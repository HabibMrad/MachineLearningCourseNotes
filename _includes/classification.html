<h2>Classification</h2>
<p><strong>Classification </strong>methods are ones that help predict which group (AKA type or class) each data point (or set of observations) belongs to. They are <strong>supervised</strong> algorithms: you train your model by giving it a bunch of examples with a known group. (<a target="_blank" href="http://articles.extension.org/pages/40214/whats-the-difference-between-a-supervised-and-unsupervised-image-classification">Sometimes</a>, people talk about unsupervised classification, but in this course, we're referring to those methods as clustering, and we'll talk about them later.) Once the model is trained, you can give it a new example (again, a set of observations) and have the model predict which group this new item most likely belongs to.</p>
<p>For example, say I have a bunch of pictures of different species of penguins, and I want to know if the computer can learn to distinguish between them. I give my classification algorithm a bunch of these images and tell it which species is in each photo. After this training period, I now have a model that maps images to penguin species, and I can give it new images and see what species it thinks is in the image. In this case, my set of observations for each example are the pixel values of the image.</p>
<h3>Binomial to Multinomial</h3>
<p>Some of the classification algorithms are <strong>binomial</strong>, meaning they can only choose between two types. If you have multiple types to choose between, a <strong>multinomial</strong> problem, you can still use these algorithms with a <strong>one-vs-rest</strong> method. Basically, you train a different model for each class, one that will predict if an example belongs to that class or not to that class (that is, to one of the other classes). You run each example through all of your models to get the probability of it being in each class.</p>
<p>Using the penguin example again, I begin training my model for Adelie penguins. All images of Adelies are labeled with a 1, and images of any other species are labeled 0. I train a second model for Gentoo penguins. When training this model, I label all Gentoos 1 and everything else 0. After I have finished with each species, I can now predict the species of a new image. First, I run it through the Adelie model to get the probability it is an Adelie. Then, I run the Gentoo predictor, and so on. At the end, I can go with the class that was given the highest probability by its model as my answer.</p>
<h3>Common Classification Algorithms</h3>
<ul>
<li><strong><a class="jump-to-section" href="#binomial-logistic-regression">Logistic Regression</a>:</strong> This method finds weights for each observation in a datapoint to find a best fit through the data, like <a class="jump-to-section" href="#linear-regression">linear regression</a>. Unlike in linear regression, it doesn't fit the weights and observations directly to the output, but uses the logit function so that it outputs probabilities. It can be made multinomial either through one-vs-rest or through a fairly straightforward extension of the logit function.</li>
<li><strong><a class="jump-to-section" href="#decision-trees">Decision Trees</a>:</strong> This method produces a model that slices up the data space in a way that is useful for classification, giving a flowchart on how to classify a new example. To do this, it looks at how well each feature can split up the data into the different classes, picks the best splits, and then does it again within each of the chosen subgroups. To predict the class of a new example, you follow along in the flow chart using the observations for that example.<br />Decision Trees are rarely used on their own, but instead as part of an ensemble like Random Forest.</li>
<li><strong><a class="jump-to-section" href="#random-forest">Random Forest</a>:</strong> This method trains many decision trees on randomly-chosen subsets of the data with randomly-chosen sets of features used for the decision points in the tree. To make a prediction, it combines the answers from all of the trees.</li>
<li><strong><a class="jump-to-section" href="#naive-bayes">Naive Bayes</a>:</strong> This method is generally used to classify text (e.g. "This email is/is not spam"). The reason for the name is that it makes naive assumptions: By assuming that all of the observations are independent of one another, this method can efficiently compute probability distributions for each class that maximize the overall likelihood of the training labels given the training observations. These assumptions are almost never true, but the classifications it predicts are often good enough.</li>
<li><strong><a class="jump-to-section" href="#support-vector-machines">Support Vector Machines</a>:</strong> This method is trying to find lines (or planes or hyperplanes) that group all examples of the same class together and separate examples of different classes from one another. Instead of looking at the distance from every point to the hyperplane (as regression does), it is trying to maximize the distance between the hyperplanes and the points nearest to them. In order words, it is looking for areas of empty space between different groups. The distances between points are computed using <strong>kernel functions</strong>, which make computation more efficient. In order to find non-linear relationships, you can use a non-linear kernel function.</li>
</ul>
<h2>Classification Validation</h2>
<p>It is always a good idea to validate your model, <a class="jump-to-section" href="#validation">as we have discussed</a>.</p>
<h3>Precision, Recall, and F-Measure</h3>
<p>For classification, you could use a fairly simple measure of what ratio of points were placed into the correct category. This is called <strong>accuracy</strong>. However, this hides a lot of potentially important details about how your model is doing.</p>
<p>Another option is to look at each class individually and see how well the model did in different aspects. For this, we put each test data point into one of four categories.</p>
<ul>
<li>true positive: the model correctly predicted this point was in this class</li>
<li>false positive: the model predicted this point was in this class, but it actually wasn't</li>
<li>true negative: the model correctly predicted this point was not in this class</li>
<li>false negative: the model predicted this point wasn't in this class, but it actually was</li>
</ul>
<p><img src="assets/img/TP_FN_FP_TN.png" alt="Square of TP, FP, TN, and FN" /></p>
<p><strong>Precision</strong> is looking to see how often our guesses about points being in this class were right: TP / (TP + FP)</p>
<p><strong>Recall</strong> (or <strong>True Positive Rate</strong> or <strong>Sensitivity</strong>) is looking to see how what fraction of points that were actually in this class were correctly identified: TP / (TP + FN)</p>
<p><strong>F-measure </strong>combines precision and recall, using the harmonic mean: 2 * (P * R) / (P + R)</p>
<p><a target="_blank" href="http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html" >More information</a></p>
<h3>The ROC Curve and AUC</h3>
<p>Another popular metric used in a lot of academic papers published on classification techniques is the Receive Operating Characteristic curve (AKA ROC curve). </p>
<p>This plots the False Positive Rate on the x-axis and the True Positive Rate on the y-axis.</p>
<ul>
<li>The True Positive Rate is the same as described above for Recall: TP / (TP + FN).</li>
<li>The False Positive Rate is FP / (FP + TN)</li>
</ul>
<p>The way the curve is created is by changing the values of the threshold probability at which you classify something as belong to the "positive class". If I am creating a model to give me the probability that it is going to rain today, I could be really cautious and carry an umbrella if the model says there is even a 10% chance of rain or I could live life on the edge and only bring my umbrella if the model was 99% sure it would rain. What threshold I pick may have a big impact on how often I am running through the rain or lugging around an unnecessary umbrella, depending on how confident and correct the model is.</p>
<p><img src="https://www.medcalc.org/manual/_help/images/roc_intro3.png" alt="Example ROC curve" width="289" height="279" /></p>
<p>(Source: <a target="_blank" href="https://www.medcalc.org/manual/roc-curves.php">https://www.medcalc.org/manual/roc-curves.php</a>)</p>
<p>The <strong>area under the ROC curve is known as the AUC</strong>. It is mathematically equivalent to the probability that, if given two random samples from your dataset, your model will assign a <strong>higher</strong> probability of belonging to the positive class to the actual positive instance. This is known as  the <strong>discrimination. </strong>The better your model is, ideally the better it can discriminate between positive instances and negative ones.</p>
<p>The dotted line in the image above would be a <strong>random classifier</strong>. That is, it would be if you built a classifier that just "flipped a coin" when it was deciding which of two samples was more likely to be in the positive class. You want to do better than random so try to make sure your models are above this dotted line!</p>
<h3>The Precision-Recall Curve </h3>
<p>The precision-recall curve is another commonly used curve that describes how good a model is.</p>
<p>It plots precision on the Y-axis and recall (also known as true positive rate and sensitivity) on the X-axis.</p>
<p><img src="http://nlp.stanford.edu/IR-book/html/htmledition/img532.png" alt="Example precision-recall-curve" width="407" height="344" /></p>
<h3>Other Classification Metrics</h3>
<p>You can see a page Carl created for other measurements of binary classification metrics: <a target="_blank" href="http://carlshan.github.io/">http://carlshan.github.io/</a></p>
