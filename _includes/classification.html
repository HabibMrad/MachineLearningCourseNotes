<h1>Classification</h1>
<p><strong>Classification </strong>methods are ones that help predict which group (AKA type or class) each data point (or set of observations) belongs to. They are <strong>supervised</strong> algorithms: you train your model by giving it a bunch of examples with a known group. (<a target="_blank" href="http://articles.extension.org/pages/40214/whats-the-difference-between-a-supervised-and-unsupervised-image-classification">Sometimes</a>, people talk about unsupervised classification, but in this course, we're referring to those methods as clustering, and we'll talk about them later.) Once the model is trained, you can give it a new example (again, a set of observations) and have the model predict which group this new item most likely belongs to.</p>
<p>For example, say I have a bunch of pictures of different species of penguins, and I want to know if the computer can learn to distinguish between them. I give my classification algorithm a bunch of these images and tell it which species is in each photo. After this training period, I now have a model that maps images to penguin species, and I can give it new images and see what species it thinks is in the image. In this case, my set of observations for each example are the pixel values of the image.</p>
<h3>Binomial to Multinomial</h3>
<p>Some of the classification algorithms are <strong>binomial</strong>, meaning they can only choose between two types. If you have multiple types to choose between, a <strong>multinomial</strong> problem, you can still use these algorithms with a <strong>one-vs-rest</strong> method. Basically, you train a different model for each class, one that will predict if an example belongs to that class or not to that class (that is, to one of the other classes). You run each example through all of your models to get the probability of it being in each class.</p>
<p>Using the penguin example again, I begin training my model for Adelie penguins. All images of Adelies are labeled with a 1, and images of any other species are labeled 0. I train a second model for Gentoo penguins. When training this model, I label all Gentoos 1 and everything else 0. After I have finished with each species, I can now predict the species of a new image. First, I run it through the Adelie model to get the probability it is an Adelie. Then, I run the Gentoo predictor, and so on. At the end, I can go with the class that was given the highest probability by its model as my answer.</p>
<h3>Common Classification Algorithms</h3>
<ul>
<li><strong><a class="jump-to-section" href="#decision-trees">Decision Trees</a>:</strong> This method produces a model that slices up the data space in a way that is useful for classification, giving a flowchart on how to classify a new example. To do this, it looks at how well each feature can split up the data into the different classes, picks the best splits, and then does it again within each of the chosen subgroups. To predict the class of a new example, you follow along in the flow chart using the observations for that example.<br />Decision Trees are rarely used on their own, but instead as part of an ensemble like Random Forest.</li>
<li><strong><a class="jump-to-section" href="#random-forest">Random Forest</a>:</strong> This method trains many decision trees on randomly-chosen subsets of the data with randomly-chosen sets of features used for the decision points in the tree. To make a prediction, it combines the answers from all of the trees. Random forest is usually the best classification technique to use, unless you are working with images or text.</li>
<li><strong><a class="jump-to-section" href="#naive-bayes">Naive Bayes</a>:</strong> This method is generally used to classify text (e.g. "This email is or is not spam"). The reason for the name is that it makes naive assumptions: By assuming that all of the observations are independent of one another, this method can efficiently compute probability distributions for each class that maximize the overall likelihood of the data. These assumptions are almost never true, but the classifications it predicts are often good enough.</li>
<li><strong><a class="jump-to-section" href="#binomial-logistic-regression">Logistic Regression</a>:</strong> This method finds weights for each observation in a datapoint to find a best fit through the data, like <a class="jump-to-section" href="#linear-regression">linear regression</a>. Unlike in linear regression, it doesn't fit the weights and observations directly to the output, but uses the logit function so that it outputs probabilities.</li>
<li><strong><a class="jump-to-section" href="#support-vector-machines">Support Vector Machines</a>:</strong> This method is trying to find lines (or planes or hyperplanes) that group all examples of the same class together and separate examples of different classes from one another. Instead of looking at the distance from every point to the hyperplane (as logistic regression does), it is trying to maximize the distance between the hyperplanes and the points nearest to them. In order words, it is looking for areas of empty space between different groups. The distances between points are computed using <strong>kernel functions</strong>, which make computation more efficient. In order to find non-linear relationships, you can use a non-linear kernel function.</li>
</ul>
