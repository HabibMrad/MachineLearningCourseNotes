<h2>Classification Validation</h2>
<p>It is always a good idea to validate your model, <a class="jump-to-section" href="#validation">as we have discussed</a>.</p>
<h3>Accuracy</h3>
<p>For classification, you can use a simple measure of what ratio of test items were placed into the correct category over the total number of test items. This is called <strong>accuracy</strong>. However, this hides a lot of potentially important details about how your model is doing. This can be particularly bad if you have <a target="_blank" href="https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28">far more examples of some classes than others</a>.</p>
<h3>Precision, Recall, and F-Measure</h3>
<p>Another option is to look at each class individually and see how well the model did in different aspects. For this, for each class, we put each test data point into one of four categories.</p>
<ul>
<li>true positive: the model correctly predicted this point was in this class</li>
<li>false positive: the model predicted this point was in this class, but it actually wasn't</li>
<li>true negative: the model correctly predicted this point was not in this class</li>
<li>false negative: the model predicted this point wasn't in this class, but it actually was</li>
</ul>
<p><img src="assets/img/TP_FN_FP_TN.png" alt="Square of TP, FP, TN, and FN" /></p>
<p><strong>Precision</strong> is looking to see how often our guesses about points being in this class were right: \(\frac{TP}{TP + FP}\)</p>
<p><strong>Recall</strong> (or <strong>True Positive Rate</strong> or <strong>Sensitivity</strong>) is looking to see how what fraction of points that were actually in this class were correctly identified: \(\frac{TP}{TP + FN}\)</p>
<p><strong>F-measure </strong>combines precision and recall, using the harmonic mean: \(2 * \frac{P * R}{P + R}\)</p>
<p><a target="_blank" href="http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html" >More information</a></p>
<h3>The ROC Curve and AUC</h3>
<p>Another popular metric used in a lot of academic papers published on classification techniques is the Receiver Operating Characteristic curve (AKA ROC curve). </p>
<p>This plots the False Positive Rate on the x-axis and the True Positive Rate on the y-axis.</p>
<ul>
<li>The True Positive Rate is the same as described above for Recall (they mean the same thing): \(\frac{TP}{TP + FN}\).</li>
<li>The False Positive Rate is \(\frac{FP}{FP + TN}\)</li>
</ul>
<p>The way the curve is created is by changing the values of the threshold probability at which you classify something as belong to the "positive class". If I am creating a model to give me the probability that it is going to rain today, I could be really cautious and carry an umbrella if the model says there is even a 10% chance of rain or I could live life on the edge and only bring my umbrella if the model was 99% sure it would rain. What threshold I pick may have a big impact on how often I am running through the rain or lugging around an unnecessary umbrella, depending on how confident and correct the model is.</p>
<p><img src="https://www.medcalc.org/manual/_help/images/roc_intro3.png" alt="Example ROC curve" width="289" height="279" /></p>
<p>(Source: <a target="_blank" href="https://www.medcalc.org/manual/roc-curves.php">https://www.medcalc.org/manual/roc-curves.php</a>)</p>
<p>The <strong>area under the ROC curve is known as the AUC</strong>. It is mathematically equivalent to the probability that, if given two random samples from your dataset, your model will assign a <strong>higher</strong> probability of belonging to the positive class to the actual positive instance. This is known as  the <strong>discrimination. </strong>The better your model is, the better it can discriminate between positive instances and negative ones.</p>
<p>The dotted line in the image above would be a <strong>random classifier</strong>. That is, it would be as if you built a classifier that just "flipped a coin" when it was deciding which of two samples was more likely to be in the positive class. You want to do better than random so try to make sure your models are above this dotted line!</p>
<p>If you'd like a video explanation of ROC and the area under the curve, <a target="_blank" href="https://www.dataschool.io/roc-curves-and-auc-explained/">this video</a> does a fine job of detailing things.</p>
<h3>The Precision-Recall Curve </h3>
<p>The precision-recall curve is another commonly used curve that describes how good a model is.</p>
<p>It plots precision on the Y-axis and recall (also known as true positive rate and sensitivity) on the X-axis.</p>
<p><img src="http://nlp.stanford.edu/IR-book/html/htmledition/img532.png" alt="Example precision-recall-curve" width="407" height="344" /></p>
<h3>Perplexity</h3>
<p>Perplexity is sometimes used in natural language processing problems rather than some of the other <a class="jump-to-section" href="#classification-validation">classification validation measures</a>. It uses the concept of entropy (<a target="_blank" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">in the information theory sense</a>) to measure how many bits of information would be needed on average to encode an item from your test set. The fewer bits needed, the better, because it implies your model is less confused about how to represent this information. The perplexity of a model on your test set is</p>
<p>$$ perplexity = 2^{\frac{-1}{n}\sum_{i=0}^n\log_{2}(p_i ) }$$ where \(n\) is the number of test points in your data and \(p_i\) is the probability your model output of the \(i^{th}\) point being in class 1.</p>
<p>For instance, if you have a classification problem, for each test point, you would see what probability \(p_i\) your model gives for the correct class (and only the correct class) for that \(i^{th}\) point and take \(log_{2}(p_i)\). Add all of these up, take the average, and raise 2 to that average.</p>
<p>See <a target="_blank" href="https://en.wikipedia.org/wiki/Perplexity">Wikipedia</a> and <a target="_blank" href="https://jamesmccaffrey.wordpress.com/2016/08/16/what-is-machine-learning-perplexity/">this blog post</a> for more information.</p>
<h3>Other Classification Metrics</h3>
<p>You can see a page Carl created for other measurements of binary classification metrics: <a target="_blank" href="http://carlshan.github.io/">http://carlshan.github.io/</a></p>
