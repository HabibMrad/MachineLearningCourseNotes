<h2>Word Vectors</h2>
<p>When working with natural language, the first thing we really have to think about is how to represent text.</p>
<p>Previously, we looked at some techniques like bag-of-words and term-frequency-inverse-document-frequency (see <a href="#text">Text</a>). The problem is that these techniques lose a lot of context that humans use, about how the words relate to one another and the order they appear in the input. We would really like our representation to capture as much of that information as possible.</p>
<p>One solution is to represent each word as a multi-dimensional vector. This vector gives the word a location that captures (at least some of) how it relates to other words, nearby to synonyms or words that play similar roles, for instance. <a target="_blank" href="https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/">The vectors found by current algorithms seem to find some complex analogous relationships</a>, such as having the vector between Moscow and Russia be very similar to the vector between Tokyo and Japan. In other words, there is a consistent movement in the vector space that connects many capitals to their countries.</p>
<p>Like many of the techniques we study, the training is a bit complicated, involving combinations of linear algebra, probability distributions, and/or convex optimization (see <a target="_blank" href="https://medium.com/ai-society/jkljlj-7d6e699895c4">here</a> or <a target="_blank" href="https://www.tensorflow.org/tutorials/word2vec">here</a> if you'd like the gritty details). The basic idea is that we're looking at what words precede and follow each word and then finding vectors that help explain that data.</p>
<p>There are two main implementations for creating word vectors at the moment: <a target="_blank" href="https://nlp.stanford.edu/projects/glove/">GloVe</a> and <a target="_blank" href="https://code.google.com/archive/p/word2vec/">word2vec</a>. There is also a library, <a target="_blank" href="https://fasttext.cc/">FastText</a> that actually looks at the characters inside of the words rather than just treating each word as something totally separate.</p>
<p>This <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningTutorials/blob/master/WordVectors.ipynb">Jupyter Notebook</a> shows an example of training vectors using word2vec.</p>
