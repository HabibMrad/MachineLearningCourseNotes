<h2>Decision Trees</h2>
<p>Decision Trees allow you to classify data by creating a branching structure that divides up the data. By following the tree for a particular input, you can arrive at a decision for that input. Below is visualization of someone making a decision around giving a loan:</p>
<p><img src="http://www.cse.unsw.edu.au/~billw/cs9414/notes/ml/06prop/id3/dtree.gif" width="512" height="290" /></p>
<p>The above image is an example of a decision tree the loan approval officer of a bank might mentally go through when trying to figure out whether to approve a loan or not. The bank may want to try to minimize the risk of giving a loan to someone who won't pay it back (this is them trying to minimize their "error function").</p>
<p>Of course, minimizing error does not always make our models fair or ethical. Basing our predictions on data from the past makes intuitive sense, but it can cause problems, because the future does not always look like the past and<em> many times we don't want to replicate the mistakes of the past. </em>We'll talk more about these ethical issues later.</p>
<h3>How do you create a Decision Tree model?</h3>
<p>Imagine you're given a large dataset of people and whether they're vampires or not. You want to build a machine learning model that is trained on this data that can best predict whether new people are vampires or not.</p>
<p>Say your dataset has the following 3 features that you can use as inputs to train your model:</p>
<ol>
<li>How long it takes the person until their skin starts burning in the sun</li>
<li>How many cloves of garlic they can eat before they have to stop</li>
<li>Whether their name is Dracula or not.</li>
</ol>
<p>A Decision Tree model is essentially trained on your data by doing the following:</p>
<ol>
<li>Select the attribute whose values will best split the datapoints into their classes: for example, select the Name-Is-Dracula attribute because everyone named Dracula in this dataset is a vampire</li>
<li>Split along that attribute: split the data into those for whom the data is Yes and No, or is less than or greater than some value</li>
<li>For each of these buckets of data (called child nodes), repeat these three steps until a stopping condition is met. Examples of stopping conditions:
<ul>
<li>There's too little data left to reasonably split</li>
<li>All the data belong to one class (e.g., all the data in the child node are not-vampires)</li>
<li>The tree has grown too large</li>
</ul>
</li>
</ol>
<h3>Measuring the best split: Gini or Entropy</h3>
<p>How do you choose the best split? The sklearn library provides two methods that you can choose between to find the best split: Gini (also known as Impurity) and Entropy (also known as Information Gain) that you can give as inputs to a Decision Tree model. Both are a measurement of how well a particular split was able to separate the classes.</p>
<p>The worst split for both Gini and Entropy are ones that split the two nodes such that classes are evenly divided between the two nodes (that is, both nodes contain 50% of Class A and 50% of Class B).
</p>
<p>
How do you choose between Gini and Entropy? Great question. There's no hard and fast rule. The general consensus is that the choice of the particular splitting function doesn't really impact the split of the data.
</p>
<p>To learn more about the difference between these, and other, splitting criteria, you can use resources such as:</p>
<ul>
<li><a target="_blank" href="https://www.slideshare.net/marinasantini1/lecture-4-decision-trees-2-entropy-information-gain-gain-ratio-55241087">https://www.slideshare.net/marinasantini1/lecture-4-decision-trees-2-entropy-information-gain-gain-ratio-55241087</a></li>
<li><a target="_blank" href="https://sebastianraschka.com/faq/docs/decisiontree-error-vs-entropy.html">https://sebastianraschka.com/faq/docs/decisiontree-error-vs-entropy.html</a></li>
</ul>
<p>You can also take a look at this <a target="_blank" href="https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/">detailed tutorial</a> on implementing decision trees.</p>
<h3>Example Code</h3>
<p>See our <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningCourseNotes/blob/master/assets/ipynb/DecisionTree.ipynb">Decision Tree Juypter Notebook</a> that analyses the iris dataset that is included in scikit-learn.</p>
