<h2>Abuse and Misuse</h2>

<p>Throughout the history of our species, people have invented a large variety of useful tools that have bettered our lives. But with each tool, from the spear to the computer, there comes the potential for abuse or misuse.</p>
<p>Some researchers demonstrated ways to “trick” machine learning algorithms to label images as gibbons. <a target="_blank" href="https://arxiv.org/abs/1312.6199">By inserting some “adversarial noise” it’s possible to perturb the algorithms to assign an incorrect label to an image.</a></p>
<p><img src="http://yourshot.nationalgeographic.com/u/ss/fQYSUbVfts-T7pS2VP2wnKyN8wxywmXtY0-Fwsgxpz-7KQoPM7C9KUxz6eybQ_f4a8jMQcqBL_pp6G-7Kndb/" alt="" width="419" height="391"></p>
<p><em>This is a gibbon.</em></p>
<p>This could potentially be used to deliberately harm others. For instance, might it be possible to somehow alter the clothing of a person so that self-driving cars do not identify them as a pedestrian? This is just one example of intentional misuse of technologies. How do we build safeguards around machine learning technologies that make it harder for them to be abused?</p>
<p>What are examples of models that we should be most worried about attackers tricking?</p>
<p> </p>
<h3>Additional Readings</h3>
<p><a target="_blank" href="https://www.theguardian.com/technology/2017/mar/13/artificial-intelligence-ai-abuses-fascism-donald-trump">Artificial Intelligence Is Ripe for Abuse, Tech Researcher Warns</a> - The Guardian</p>
