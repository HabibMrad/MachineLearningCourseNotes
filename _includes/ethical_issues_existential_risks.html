<h2>Existential Risks</h2>

<p>One of the big overarching concerns around machine learning is that it could eventually produce general artificial intelligence. There exist a plethora of movies/books hailing AI bringing about a Terminator-esque doomsday scenario. Counterposing these media, other movies and books proclaim that AI will bring about an utopian world.</p>
<p><em>This is a bad robot ...</em></p>
<p><img src="http://blogs-images.forbes.com/markhughes/files/2016/01/Terminator-2-1200x873.png" alt="Terminator" width="75%"></p>
<p> </p>
<p><em>And this is a nice robot! But which one will prevail?</em><img src="http://coubvideos.com/wp-content/uploads/2015/03/w8.jpg" alt="Wall-E" width="75%"></p>
<p>It’s not obvious which outcome, if either, will occur in a world in which we have AI.</p>
<p>Due to the rapid progress of fields like Deep Learning, there is growing academic and industry work related around curtailing the existential risk of AI. For example, the Open Philanthropy Project recently published a report indicating that they’d be investing significant resources towards funding initiatives that reduced the chance of AI-caused existential risk. (Open Philanthropy Project report: <a target="_blank" href="http://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity#Sec7">link</a>)</p>
<p>Below are some areas of concern that have been expressed.</p>
<ul>
  <li>
    <strong>The Superintelligence problem:</strong> AI that is capable enough will become self-augmenting, making itself more and more powerful despite any safeguards we put around it.
  </li>
  <li>
    <strong>The values embedding problem:</strong> We want an AI to be “good” but we don’t have a rigorous, clear and universally agreed definition of “good” that is easily expressible in code. This problem may be made worse by the following <a target="_blank" href="http://www.nickbostrom.com/superintelligentwill.pdf">two ideas by philosopher Nick Bostrom</a>:
    <ul>
      <li>
        <strong>The orthogonality thesis: </strong>Intelligence and morality are orthogonal to each other. That is to say that the two attributes don’t have a strong correlation with one another. The implication of this is that an incredibly advanced AI cannot be trusted to naturally deduce how to be “good."
      </li>
      <li>
        <strong>Instrumental convergence thesis: </strong>Despite the above idea, Bostrom also argues that irrespective of any AI’s goals, there are a number of instrumental or intermediate goals that all AIs will pursue, since these intermediate goals are likely to be useful for a wide range of goals. Examples of instrumental goals include e.g., self-preservation and cognitive enhancement.
      </li>
    </ul>
  </li>
  <li>
    <strong>Unintended consequences: </strong>You command your AI to build as many paperclips as it can. To maximize this goal, your AI turns the entire planet into scrap material that it can use to create as many paperclip factories as possible.
  </li>
</ul>
<h3>Additional Readings</h3>
<ol>
  <li><a target="_blank" href="https://futureoflife.org/background/benefits-risks-of-artificial-intelligence/">Benefits and Risk of AI</a> - University of Oxford's Future of Humanity Institute</li>
  <li>Open Philanthropy’s research around AI: <a target="_blank" href="http://www.openphilanthropy.org/blog/potential-risks-advanced-artificial-intelligence-philanthropic-opportunity">link</a></li>
  <li>The Value Learning Problem: <a target="_blank" href="https://intelligence.org/files/ValueLearningProblem.pdf">link</a>. Nate Sores of the Machine Intelligence Research Institute (MIRI)</li>
  <li><a target="_blank" href="https://docs.google.com/document/d/16Te6HnZN2OEviYFA-42Tf9Pal_Idovtgr5Y1RGEPW_g/edit#">Landscape of current work on potential risks from advanced AI</a> - Open Philanthropy Project</li>
  <li>The Ethics of Artificial Intelligence by Nick Bostrom and Eliezer Yudkowsky: <a target="_blank" href="https://intelligence.org/files/EthicsofAI.pdf">link</a></li>
  <li><a target="_blank" href="https://www.goodreads.com/book/show/20527133-superintelligence?from_search=true">Superintelligence by Nick Bostrom</a></li>
  <li><a target="_blank" href="http://lesswrong.com/lw/kkn/a_visualization_of_nick_bostroms_superintelligence/">Visualization of ideas</a> from the book - LessWrong</li>
</ol>
