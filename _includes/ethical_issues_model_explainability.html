<h2>Model Explainability</h2>
<p>Even simple models can be notoriously hard to interpret.</p>
<p>For instance, in this altered version of our very simple "plant A" vs. "plant B" <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningCourseNotes/blob/master/assets/ipynb/LogisticRegressionOddBehavior.ipynb">logistic regression</a>, negative weights might not always mean that an increase in that feature makes the item less likely to be in the target class. It could just mean that the other features were much more important predictors. If we use the wrong model (as we saw when we try to fit a linear model to a parabolic dataset in the <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningCourseNotes/blob/master/assets/ipynb/LinearRegression.ipynb">linear regression exercise</a>), the weights might be entirely meaningless.</p>
<p>This problem only gets worse as the models get more complicated and less well understood, like the neural networks we'll be looking at soon. If a machine learning model decides that <a target="_blank" href="https://hbr.org/2016/12/hiring-algorithms-are-not-neutral">you shouldn't get a job</a>, that <a target="_blank" href="http://www.npr.org/sections/alltechconsidered/2017/03/31/521946210/will-using-artificial-intelligence-to-make-loans-trade-one-kind-of-bias-for-anot">you shouldn't get a loan</a>, that <a target="_blank" href="https://www.bloomberg.com/view/articles/2017-05-15/don-t-grade-teachers-with-a-bad-algorithm">you are a bad teacher</a>, or that <a target="_blank" href="https://www.popsci.com/nsas-skynet-might-not-be-able-to-tell-what-makes-terrorist">you are a terrorist</a>, how do you argue with that? There might be no one thing that you can point to for where it went wrong. You may not be very comforted to hear that the model predicts these things quite well, and that you might just be in the small percentage of misclassifications. This is only made more complicated by the fact that <a target="_blank" href="https://www.nytimes.com/2017/08/24/nyregion/showing-the-algorithms-behind-new-york-city-services.html">these models are often not made public</a>.</p>
<p>There is some great research in trying to explain what models are learning (see <a target="_blank" href="http://yosinski.com/deepvis">this</a> or <a target="_blank" href="https://homes.cs.washington.edu/~marcotcr/blog/lime/">this</a>), but it still has a long way to go.</p>
