<h2>Responsibility</h2>

<p>One question that touches upon all aspects of the above risks around machine learning is that of culpability: who is responsible for the negative outcomes that arise from these technologies? Is proper use of machine learning <a target="_blank" href="https://towardsdatascience.com/artificial-intelligence-do-stupid-things-faster-with-more-energy-379aa6bac220">something that we can teach or assess?</a></p>
<p>If a self-driving car injures another person, who is liable? The engineering team that implemented and tested the algorithm? The self-driving car manufacturer? The academics who invented the algorithm? Everyone? No one? <a target="_blank" href="http://moralmachine.mit.edu/">How do we even decide the right thing to do in bad situations?</a><br></p>
<p>As another example of how hard it can be to assign blame, <i>“a Swiss art group created an automated shopping robot with the purpose of committing random </i><i><a target="_blank" href="http://www.pcworld.com/article/2046227/meet-darknet-the-hidden-anonymous-underbelly-of-the-searchable-web.html">Darknet</a> purchases. The robot managed to purchase several items, including a Hungarian passport and some Ecstasy pills, before it was “arrested” by Swiss police. The aftermath resulted in no charges against the robot nor the artists behind the robot.”</i> <i>Source: </i><a target="_blank" href="https://techcrunch.com/2015/08/22/artificial-intelligence-legal-responsibility-and-civil-rights/">Artificial Intelligence, Legal Responsibility and Civil Rights</a></p>
<p>In an article by Matthew Scherer published in the Harvard Law Review titled: <i>Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies (</i><a target="_blank" href="http://ieet.org/index.php/IEET/more/danaher20150707">link to analysis</a>), Scherer posed a number of different challenges that related to machine learning regulation:</p>
<ol>
  <li>
    <strong>The Definitional Problem</strong>: As paraphrased in an analysis by John Danaher, “If we cannot adequately define what it is that we are regulating, then the construction of an effective regulatory system will be difficult. We cannot adequately define ‘artificial intelligence’. Therefore, the construction of an effective regulatory system for AI will be difficult.”
  </li>
  <li>
    <strong>The Foreseeability Problem</strong>:  Traditional standards for legal liability describe that if some harm occurs, someone is liable for that harm if the harm was reasonably foreseeable. However, AI systems may act in ways that even the creators of the system may not foresee.
  </li>
  <li>
    <strong>The Control Problem</strong>: This has two versions.
    <ol>
      <li>Local Control Loss: Humans who were assigned legal responsibility over the AI no longer can hold control over it.</li>
      <li>Global Control Loss: No humans control the AI.</li>
    </ol>
  </li>
  <li>
    <strong>The Discreetness Problem</strong>: “AI research and development could take place using infrastructures that are not readily visible to the regulators. The idea here is that an AI program could be assembled online, using equipment that is readily available to most people, and using small teams of programmers and developers that are located in different areas. Many regulatory institutions are designed to deal with largescale industrial manufacturers and energy producers. These entities required huge capital investments and were often highly visible; creating institutions than can deal with less visible operators could prove tricky.”
  </li>
  <li>
    <strong>The Diffuseness Problem</strong>: “This is related to the preceding problem. It is the problem that arises when AI systems are developed using teams of researchers that are organisationally, geographically, and perhaps more importantly, jurisdictionally separate. Thus, for example, I could compile an AI program using researchers located in America, Europe, Asia and Africa. We need not form any coherent, legally recognisable organisation, and we could take advantage of our jurisdictional diffusion to evade regulation.”
  </li>
  <li>
    <strong>The Discreteness Problem</strong>: “AI projects could leverage many discrete, pre-existing hardware and software components, some of which will be proprietary (so-called ‘off the shelf’ components). The effects of bringing all these components together may not be fully appreciated until after the fact.”
  </li>
  <li>
    <strong>The Opacity Problem:</strong> “The way in which AI systems work may be much more opaque than previous technologies. This could be ... because the systems are compiled from different components that are themselves subject to proprietary protection. Or it could be because the systems themselves are creative and autonomous, thus rendering them more difficult to reverse engineer. Again, this poses problems for regulators as there is a lack of clarity concerning the problems that may be posed by such systems and how those problems can be addressed.”
  </li>
</ol>
<p>Who is responsible for unintended negative outcomes of machine learning algorithms?</p>
