<h1>Generative Models</h1>
<p>A <strong>generative model</strong> is one whose goal is to create outputs that look like the inputs it has trained on. This is an unsupervised problem, because there aren't specific outputs that we are looking to get from each input; we just want the outputs to look like they could have been real inputs.</p>
<h3>Uses</h3>
<p>Why would we ever want to do this? Creating <a target="_blank" href="https://deepdreamgenerator.com/">mash-ups of two paintings</a> or <a target="_blank" href="http://aiweirdness.com/post/182633984547/gancats">cat</a> <a target="_blank" href="https://github.com/aleju/cat-generator">photos</a> or <a target="_blank" href="https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/">slightly surreal Harry Potter chapters</a> or <a target="_blank" href="https://blog.openai.com/better-language-models/">disturbingly realistic paragraphs</a> or <a target="_blank" href="https://www.youtube.com/watch?v=LY7x2Ihqjmc">quite surreal sci-fi shorts</a> is certainly fun, but are there other applications? Yes! The thing is, in order to generate output well, a generative model needs to learn the underlying patterns that are in the training data. Underlying patterns are exactly the things we are often trying to learn in our supervised machine learning problems -- but these models don't need labeled training data!</p>
<p>One general thing that generative models can be used for is pre-training data. We talked about this idea when looking at word vectors: training on a large amount of unlabeled data might help get our model ready to learn more quickly from our (almost certainly smaller) labeled training set.</p>
<p>Generative models can also help augment the training data, providing additional realistic inputs that can help a supervised model learn better, as done in this <a target="_blank" href="https://arxiv.org/pdf/1709.01648.pdf">medical risk prediction study</a>. This technique is called <a target="_blank" href="http://scikit-learn.org/stable/modules/label_propagation.html">semi-supervised learning</a>.</p>
<p>And finally, sometimes we would really like to be able to generate (our best guesses at) data that is hard to get based on data that is easier to get. An example is <a target="_blank" href="https://arxiv.org/abs/1612.05362">this study that generated guesses at CT scans (which involves radiation exposure for patients) based on MRI scans (which are much safer for patients). </a></p>
<h3>Training</h3>
<p>Training can be a little tricky. If we give our model an input and tell it that what we want is the same output, it could just learn to exactly copy whatever it is given as an input.</p>
<p>One way to make sure the model doesn't just copy is to make sure that at least one of the layers in the network is too small to hold all of the input data. As information flows through this layer, it <em>has </em>to be compressed into some more fundamental pattern.</p>
<p>Another trick was to add some noise to the training inputs and tell the model to recover the actual input as its output. We might randomly change some of the pixels in an image or characters in a text, things that the model has to learn to fix.</p>
<p>Perhaps the most promising technique currently is the <strong>generative adversarial network</strong>. This essentially uses two networks: one network is a generator and the other is a discriminator. The discrimator's job is to learn to distinguish between real samples and samples created by the generator network. The crucial part is that backpropagation training goes through <em>both </em>networks. So the generator learns information about why the discriminator was not fooled by one of its images.</p>
<h3>Further reading</h3>
<ul>
<li>
<a target="_blank" href="https://blog.openai.com/generative-models/">An overview of generative models, different ways of implementing them, and future uses</a>.</li>
<li>
<a target="_blank" href="https://towardsdatascience.com/deep-generative-models-25ab2821afd3">Further explanation of how different generative models work, with a bit of the math</a>.</li>
</ul>
<h3>Suggested Exercises</h3>
<p>For some GAN exercises, see <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningTutorials/blob/master/GAN.ipynb">this Jupyter notebook</a></p>

<h2>Generative Validation</h2>
Since the goal is to create novel output, validating generative models can be tricky. Frequently, the validation is done with human input: does the output from one model look "more real" to humans than the output from another model? In some specific cases, there might be measurements of quality that you can use, such as <a href="https://www.aclweb.org/anthology/W19-2311">these</a> <a target="_blank" href="https://arxiv.org/pdf/1802.01886.pdf">techniques</a> for measuring the quality of generated text.
