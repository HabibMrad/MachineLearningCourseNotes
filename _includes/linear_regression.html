<h2>Linear Regression</h2>
<p><strong>Ordinary Least Squares Linear Regression (OLS)</strong> is a tool for predicting a continuous numerical output from numerical inputs. It's one of the foundational tools in statistical modeling. It's used to make predictions on new data, but it's also used to estimate the "importance" of certain variables in predicting the outcome.</p>
<p>As an example, you might be looking to predict the age of a tree based on its height and circumference. Linear regression may work well for this, because all of the inputs are numbers and it is fine to predict that a tree is 5.27 years old. In contrast, linear regression would not work for predicting the type of tree, because the output is not numeric. Even if you assigned numbers to each type of tree, the output would not be continuous: what would it mean to predict 5.27, a quarter of the way between elm and spruce?</p>
<h3>Training</h3>
<p>The OLS model is constructed (or "<strong>trained</strong>") by giving it sample inputs and the corresponding outputs. These outputs are called the <strong>labels</strong> of the data. In the tree example above, we would have the data for a bunch of trees, with each input having two numbers, one for the height and one for the circumference, and a matching output that gives the known age of the tree.</p>
<p>The linear regression training algorithm finds <strong>weights</strong> or <strong>parameters</strong> for each input so as to minimize the sum of the <em>squared</em> errors between the predicted outputs and the desired outputs. This gives a best-fit surface through the data. In this case, the sum of squared errors is the function that you are trying to minimize when you estimate the linear model. The Python code for it might look like this:</p>
<pre>sq_err = sum([(predicted - actual)**2 for predicted, actual in examples])</pre>
<p>The function you are minimizing is known as the <strong>cost function</strong> or <strong>loss function</strong>. You can watch this <a target="_blank" href="https://www.youtube.com/watch?time_continue=1&v=6OvhLPS7rj4">Khan Academy video</a> if you would like some more details about linear regression and the idea of squared errors.</p>
<p>Training a linear regression model involves linear algebra and calculus, which most of you have not seen, but if you would like the details, see <a target="_blank" href="http://www.deeplearningbook.org/contents/ml.html">section 5.1.4 of this book</a> or read <a target="_blank" href="https://www.stat.auckland.ac.nz/~lee/330/lects/762slides1.pdf">this online presentation</a>.</p>
<h3>Prediction</h3>
<p>Armed with these trained weights, we can now look at a new input set, multiply each part of the input by its corresponding weight, and get a predicted output. For instance, for the data set shown in the graph below, if my new input is x=4, we're going to guess that y will be 9.2.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Linear_least_squares_example2.svg/2000px-Linear_least_squares_example2.svg.png" alt="Best fit line with dataset points." width="332" height="326" /></p>
<p>If we have two inputs instead of just one, then we get something that looks like the graph below; we are finding a best fit plane instead of a line. Now, we give an x<sub>1</sub> and x<sub>2</sub> in order to predict a y. We can continue adding as many inputs per data point as we want, increasing the dimensionality. It becomes a lot harder to visualize, but the math stays the same.</p>
<p><img src="assets/img/3D_Linear_Regression.png" alt="Best fit plane with dataset points." width="381" height="280" /></p>
<p>At the end of the training in our tree example, we would have two weights, one for height and one for circumference, and an intercept constant. If we had the measurements for some tree, we would multiply each measurement by its weight and then add that together with the intercept to get the model's prediction for the age of that tree.</p>
<h3>Limitations</h3>
<p>One big limitation of linear regression is that it assumes that there is a linear relationship between the inputs and the output. If this assumption is violated but you try to build a linear model anyway, it can lead to some very odd conclusions, as illustrated by the infamous <a target="_blank" href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet">Anscombe's Quartet</a> -- all of the datasets below have the exact same best-fit line.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Anscombe%27s_quartet_3.svg/1280px-Anscombe%27s_quartet_3.svg.png" alt="Anscombe's Quartet graphs" width="475" height="346" /></p>
<h3>Example Code</h3>
<p>Our <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningCourseNotes/blob/master/assets/ipynb/LinearRegression.ipynb">Linear Regression Jupyter Notebook</a> uses the scikit-learn Python library's linear model module to perform the regression.</p>
<p>You can find another basic example of using that library at <a target="_blank" href="http://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares">http://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares</a> and full documentation at <a target="_blank" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html</a></p>
