<h1>Linear Regression</h1>
<p><strong>Ordinary Least Squares Linear Regression (OLS)</strong> is a tool for predicting a continuous numerical output from numerical inputs. It's one of the foundational tools in statistical modeling. It's used to make predictions on new data, but it's also used to estimate the "importance" of certain variables in predicting the outcome.</p>
<p>You have probably seen examples of lines before. They have the equation \(y = mx + b\) where \(m\) is the <i>slope</i> and \(b\) is the <i>intercept</i> of the line.</p>
<p>The goal of a linear regression model is to find the best model parameters (meaning the best values for \(m\) and \(b\)) to get a line that fits the datapoints.</p>
<p>If you have more than one <em>feature</em> in your data, that means more than one \(m\). With two features, you are trying to find a plane that fits the datapoints. As you add more features (I've worked with models that had about 40,000!), you are then trying to find the best surface for your datapoints.</p>
<p>As an example, you might be looking to predict the age of a tree based on two features: its height and its circumference. Linear regression may work well for this, because all of the inputs are numbers and the output is a number as well. It is fine to predict that a tree is 5.27 years old.</p>
<p>In contrast, linear regression would not work for predicting the type of tree, because the output is not numeric. Even if you assigned numbers to each type of tree, the output would not be continuous: what would it mean to predict 5.27, a quarter of the way between elm and spruce? This would be called a classification problem rather than a regression.</p>
<h3>Training (fitting) your model</h3>
<p>So how do you find the "best" parameters of a model? What does it mean to be "best" anyway?</p>
<p>The Linear Regression model is constructed (or <strong>trained</strong>) by giving it sample inputs and the corresponding outputs. The outputs are called the <strong>labels</strong> of the data. In the tree example above, we would have the data for a bunch of trees, with each input having two numbers, one for the height and one for the circumference, and a matching output that gives the known age of the tree.</p>
<p>The linear regression training algorithm finds <strong>weights</strong> for each feature of the input so as to minimize the sum of the <em>squared</em> errors between the predicted outputs and the desired outputs. This gives a best-fit surface through the data. In this case, the sum of squared errors is the function that you are trying to minimize when you estimate the linear model. The squared error for each input datapoint is $$squaredError = (predictedValue - actualValue)^2$$</p>
<p>Linear Regression will attempt to minimize the <b>sum</b> of all of the \(squaredErrors\) by calculating the error for each datapoint and then summing them.</p>
<p>The prediction for any particular input will be the sum of all of the values for each feature multipled by the weight for that feature, or $$predictedValue = intercept + sum(featureVal_1 * featureWeight_1 + featureVal_2 * featureWeight_2...)$$</p>
<p>In other words, for a model with one feature, you are trying to find the parameters \(m\) and \(b\) so that the total error is minimized. You want to find a line that minimizes the sum of the squared distances between each point and the line.</p>
<p>The function you are minimizing is known as the <strong>cost function</strong> or <strong>loss function</strong>. You can watch this <a target="_blank" href="https://www.youtube.com/watch?time_continue=1&v=6OvhLPS7rj4">Khan Academy video</a> if you would like some more details about linear regression and the idea of squared errors.</p>
<p>Training a linear regression model involves linear algebra and calculus, which most of you have not seen, but if you would like the details, see <a target="_blank" href="http://www.deeplearningbook.org/contents/ml.html">section 5.1.4 of this book</a> or read <a target="_blank" href="https://www.stat.auckland.ac.nz/~lee/330/lects/762slides1.pdf">this online presentation</a>.</p>
<h3>Prediction</h3>
<p>Armed with these trained weights, we can now look at a new input set, multiply each part of the input by its corresponding weight, and get a predicted output. For instance, for the data set shown in the graph below, if my new input is x=4, we're going to guess that y will be 9.2.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Linear_least_squares_example2.svg/2000px-Linear_least_squares_example2.svg.png" alt="Best fit line with dataset points." width="332" height="326" /></p>
<p>If we have two inputs instead of just one, then we get something that looks like the graph below; we are finding a best fit plane instead of a line. Now, we give an x<sub>1</sub> and x<sub>2</sub> in order to predict a y. We can continue adding as many inputs per data point as we want, increasing the dimensionality. It becomes a lot harder to visualize, but the math stays the same.</p>
<p><img src="assets/img/3D_Linear_Regression.png" alt="Best fit plane with dataset points." width="381" height="280" /></p>
<p>At the end of the training in our tree example, we would have two weights, one for height and one for circumference, and an intercept constant. If we had the measurements for some tree, we would multiply each measurement by its weight and then add that together with the intercept to get the model's prediction for the age of that tree.</p>
<h3>Limitations</h3>
<p>One big limitation of linear regression is that it assumes that there is a linear relationship between the inputs and the output. If this assumption is violated but you try to build a linear model anyway, it can lead to some very odd conclusions, as illustrated by the infamous <a target="_blank" href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet">Anscombe's Quartet</a> -- all of the datasets below have the exact same best-fit line.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/ec/Anscombe%27s_quartet_3.svg/1280px-Anscombe%27s_quartet_3.svg.png" alt="Anscombe's Quartet graphs" width="475" height="346" /></p>
<h3>Example Code</h3>
<p>You can find a basic example of using the scikit-learn library at <a target="_blank" href="http://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares">http://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares</a> and full documentation at <a target="_blank" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html</a></p>
<h3>Exercises</h3>
<p>Our <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningCourseNotes/blob/master/assets/ipynb/LinearRegression.ipynb">Linear Regression Jupyter Notebook</a> uses the scikit-learn Python library's linear model module to perform the regression and includes questions and suggested exercises. See the <a class="jump-to-section" href="#setup-and-tools">Setup and Tools</a> section to get your laptop set up to run Jupyter notebooks with the required libraries.</p>
