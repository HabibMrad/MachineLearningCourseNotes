<h2>Naive Bayes</h2>
<p>Imagine the following scenarios: You have a bunch of pieces of text. Maybe you have wikipedia pages and you want to know which categories they fall into. Or you have a bunch of emails and you want to know which ones are spam. You have a collection of documents and you want to know if any show evidence of people planning a crime. These are classification questions: we want to use the words in each document to decide which category it falls into.</p>
<p>When doing text classification, you have a lot of features, because each word is a different feature. In our earlier examples, we were looking at datasets that had 2-4 different features (things like height, petal length, sepal width, and so on). <a target="_blank" href="https://www.languagemonitor.com/top-words-of-the-year/no-of-words/">As of January 1 2017, the estimate for the number of words in the English language was 1,041,257.</a> That's a lot of potential features!</p>
<p>For these types of <strong>natural language processing</strong> problems, we want a technique that can very quickly train a model with a large number of features. Many of the other classification types take a lot more time as the number of features go up, but Naive Bayes scales linearly in the number of features. That is, if there are twice as many features, it takes twice as long, and not, say, four or eight times as long.</p>
<h3>Training: Calculating Probabilities</h3>
<p>For these questions, we can provide an answer if we have a good enough answer to the question "What is the probability that my document belongs in class C, given the contents X of the document?"</p>
<p>We express "What is the probability of C given X?" in mathematical shorthand as:</p>
<p><center>P(C | X)</center></p>
<p>This is called the <strong>posterior</strong> probability. Bayes Theorem is a famous theorem in statistics and probability. It tells us that the probability of C given X is equal to the probability of C <em>times</em> the probability of X given C, <em>scaled by</em> the probability of X, expressed as</p>
<p><img src="http://chem-eng.utoronto.ca/~datamining/dmc/images/Bayes_rule.png"></p>
<p>P(C) is called the <strong>prior</strong>. It is the probability that a message will be in that class if you don't know anything about it, your prior information about the classes of messages before you started learning from your data. Commonly, you would use an <strong>uninformative prior</strong>, making it 1/<em>n</em>, where <em>n</em> is the number of classes, for every class. This would mean that your prior belief is that each class is equally likely for a document if you don't know what is in the document. If you feel your dataset is a very representative of everything you might see in the future, then you could instead use the fraction of documents that are in the class in the dataset (or perhaps a previous dataset), giving yourself a more informed prior. For example, if half of the documents were in class 1, and a quarter each in classes 2 and 3, then you would use 1/2 as the prior for class 1 and 1/4 for classes 2 and 3.</p>
<p>P(X | C) is called the <strong>likelihood</strong>. This is the probability that we would see the words X if the document was in class C. This is the hard part to compute. If we wanted to do it accurately, then we would be trying to compute a giant joint probability, where essentially we would need to figure out what the probability of a specific word is given the class of the document <em>and every other word the document.</em> Instead, we can make the <em>naive</em> (and almost certainly wrong) assumption that all of the words are independent of each other (we're saying that if <em>machine</em> is in the document, <em>learning</em> is no more or less likely to be!). Making this assumption means that we can just multiply each P(x | C) -- where x is a single word -- to get the overall P(X | C). P(x | C) might be estimated simply, perhaps as the number of documents in class C that contain word x over the number of documents in class C or as the number of times that word x appears in a document of class C over the total number of words in all documents of class C. We will talk about more ways to do this when we cover dataset processing later in the semester.</p>
<p>P(X) is called the <strong>evidence</strong>. In general, in any document, how likely are you to see this set of words? Maybe it's quite likely to see a particular word (like <em>the</em>) in some class of documents, but if you're also likely to see that word in many classes, then it shouldn't increase the probability of any of those classes as much as it would if it were only seen mostly in one class. Again, this would take awhile to truly calculate, as you are essentially calculating P(X | C) for every C. However, making the same naive assumption as above, it becomes fairly easy. Note: While this term is important for getting out a probability for P(C | X), you don't need it if all you want to know is what class the model thinks your word is in. You can just calculate the numerator for each class to see which one is the biggest. The sci-kit learn documentation warns that Naive Bayes is a decent classifier, but the probabilities are usually not very good, so it might not be worth it to do the full calculation.</p>
<h3>Predicting</h3>
<p>What we want to return is the most likely class of a particular document. We can use the equation above to calculate our estimate for the numerator of P(C | X) for each class C, and then our answer is the class whose numerator is the highest. This is called <strong>maximum a posteriori (MAP)</strong> estimation.</p>

<h3>More information</h3>
<ul>
<li><a target="_blank" href="http://scikit-learn.org/stable/modules/naive_bayes.html">http://scikit-learn.org/stable/modules/naive_bayes.html<br></a></li>
<li><a target="_blank" href="https://www.analyticsvidhya.com/blog/2015/09/naive-bayes-explained/">https://www.analyticsvidhya.com/blog/2015/09/naive-bayes-explained/</a></li>
<li><a target="_blank" href="https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf">https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf</a></li>
</ul>

<h3>Validation: Perplexity</h3>
<p>Perplexity is sometimes used in natural language processing problems rather than some of the other <a class="jump-to-section" href="#classification-validation">classification validation measures</a>. It uses the concept of entropy (<a target="_blank" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">in the information theory sense</a>) to measure how many bits of information would be needed on average to encode an item from your test set. The fewer bits needed, the better, because it implies your model is less confused about how to represent this information. The perplexity of a model on your test set is</p>
<p><center>2 ^ (-1/(num_test_points) * sum_over_test_points( log<sub>2</sub>model_prob(test_point) ) )</center></p>
<p>For instance, if you have a classification problem, for each test point, you would see what probability your model gives for the correct class (and only the correct class) for that point and take the log<sub>2</sub> of that. Add all of these up, take the average, and raise 2 to that average.</p>
<p>See <a target="_blank" href="https://en.wikipedia.org/wiki/Perplexity">Wikipedia</a> and <a target="_blank" href="https://jamesmccaffrey.wordpress.com/2016/08/16/what-is-machine-learning-perplexity/">this blog post</a> for more information.</p>
