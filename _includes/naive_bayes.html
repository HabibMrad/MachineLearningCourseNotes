<h2>Naive Bayes</h2>
<p>Imagine the following scenarios: You have a bunch of pieces of text. Maybe you have wikipedia pages and you want to know which categories they fall into. Or you have a bunch of emails and you want to know which ones are spam. You have a collection of documents and you want to know if any show evidence of people planning a crime. These are classification questions: we want to use the words in each document to decide which category it falls into.</p>
<p>When doing text classification, you can end up with a lot of features, if each word is a different feature. In our earlier examples, we were looking at datasets that had 2-4 different features (things like height, petal length, sepal width, and so on). <a target="_blank" href="https://www.languagemonitor.com/top-words-of-the-year/no-of-words/">As of January 1 2019, an estimate for the number of words in the English language was 1,052,010.</a> That's a lot of potential features!</p>
<p>For these types of <strong>natural language processing</strong> problems, we want a technique that can very quickly train a model with a large number of features. Many of the other classification types take a lot more time as the number of features go up, but Naive Bayes scales linearly in the number of features. That is, if there are twice as many features, it takes twice as long, and not, say, four or eight times as long.</p>
<h3>Training: Calculating Probabilities</h3>
<p>For these questions, we can provide an answer if we have a good enough answer to the question "What is the probability that my document belongs in class C, given the contents X of the document?"</p>
<p>We express "What is the probability of C given X?" in mathematical shorthand as:
\[ P(C | X) \]
</p>
<p>This is called the <strong>posterior</strong> probability. Bayes Theorem is a famous theorem in statistics and probability. It tells us that the probability of C given X is equal to the probability of C times the probability of X given C, scaled by the probability of X, expressed as
\[P(C|X) = \frac{P(C)P(X|C)}{P(X)} \]
</p>
<p>\(P(C)\) is called the <strong>prior</strong>. It is the probability that a message will be in that class if you don't know anything about it, your prior information about the classes of messages before you started learning from your data. Commonly, you would use an <strong>uninformative prior</strong>, making it \(\frac{1}{n}\), where \(n\) is the number of classes, for every class. This would mean that your prior belief is that each class is equally likely for a document if you don't know what is in the document. If you feel your dataset is a very representative of everything you might see in the future, then you could instead use the fraction of documents that are in the class in the dataset (or perhaps a previous dataset), giving yourself a more informed prior. For example, if half of the documents were in class 1, and a quarter each in classes 2 and 3, then you would use \(\frac{1}{2}\) as the prior for class 1 and \(\frac{1}{4}\) for classes 2 and 3.</p>
<p>\(P(X | C)\) is called the <strong>likelihood</strong>. This is the probability that we would see the words X if the document was in class C. This is the hard part to compute. If we wanted to do it accurately, then we would be trying to compute a giant joint probability, where essentially we would need to figure out what the probability of a specific word is given the class of the document <em>and every other word the document.</em> Instead, we can make the naive (and almost certainly wrong) assumption that all of the words are independent of each other (we're saying that if <em>machine</em> is in the document, <em>learning</em> is no more or less likely to be!). Making this assumption means that we can just multiply each \(P(x | C)\) -- where \(x\) is a single word -- to get the overall \(P(X | C)\). \(P(x | C)\) might be estimated simply, perhaps as the number of documents in class \(C\) that contain word \(x\) over the number of documents in class \(C\) or as the number of times that word \(x\) appears in a document of class \(C\) over the total number of words in all documents of class \(C\), but usually we would use a measure that takes into account the length of documents and how often that word normally appears. We will talk about this and other ways to calculate \(P(x | C) \) when we cover dataset processing later in the semester.</p>
<p>\(P(X)\) is called the <strong>evidence</strong>. In general, in any document, how likely are you to see this set of words? Maybe it's quite likely to see a particular word (like <em>the</em>) in some class of documents, but if you're also likely to see that word in many classes, then it shouldn't increase the probability of any of those classes as much as it would if it were only seen mostly in one class. Again, this would take awhile to truly calculate, as you are essentially calculating \(P(X | C)\) for every \(C\). However, making the same naive assumption as above, it becomes fairly easy. Note: While this term is important for getting out a probability for \(P(C | X)\), you don't need it if all you want to know is what class the model thinks your word is in. You can just calculate the numerator for each class to see which one is the biggest. The sci-kit learn documentation warns that Naive Bayes is a decent classifier, but the probabilities are usually not very good, so it might not be worth it to do the full calculation.</p>
<h3>Predicting</h3>
<p>What we want to return is the most likely class of a particular document. We can use the equation above to calculate our estimate for the numerator of \(P(C | X)\) for each class \(C\), and then our answer is the class whose numerator is the highest. This is called <strong>maximum a posteriori (MAP)</strong> estimation.</p>

<h3>More information</h3>
<ul>
<li><a target="_blank" href="http://scikit-learn.org/stable/modules/naive_bayes.html">http://scikit-learn.org/stable/modules/naive_bayes.html<br></a></li>
<li><a target="_blank" href="https://www.analyticsvidhya.com/blog/2015/09/naive-bayes-explained/">https://www.analyticsvidhya.com/blog/2015/09/naive-bayes-explained/</a></li>
<li><a target="_blank" href="https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf">https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf</a></li>
</ul>

<h3>Example Code</h3>
<p>See our <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningCourseNotes/blob/master/assets/ipynb/NaiveBayes.ipynb">Naive Bayes Juypter Notebook</a> that analyses the 20 newsgroups dataset that is included in scikit-learn.</p>
