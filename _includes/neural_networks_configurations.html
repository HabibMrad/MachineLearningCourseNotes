<h2>Neural Network Configurations</h2>
<p>There are <a target="_blank" href="http://www.asimovinstitute.org/neural-network-zoo/">many types of neural network configurations</a>, but we'd like to focus on just a few main types:
    <ul>
        <li><a class="jump-to-section" href="#fully-connected-feed-forward-networks">Fully-Connected Feed-Forward Networks</a></li>
        <li><a class="jump-to-section" href="#convolutional-neural-networks">Convolutional Networks</a></li>
        <li><a class="jump-to-section" href="#recurrent-neural-networks">Recurrent Networks</a></li>
    </ul>
</p>
<h2>Fully-Connected Feed-Forward Networks</h2>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz41.png" alt="fully connected feedforward network" width="488" height="243"></p>
<p>(image from <a target="_blank" href="http://neuralnetworksanddeeplearning.com/chap6.html">http://neuralnetworksanddeeplearning.com/chap6.html</a>)</p>
<p>This is the type of network we <a href="#neural-networks">first talked about</a>. Every node is connected to every other node in the layers next to it. Each connection has its own separate weight. Information always flows forward, from nodes on the left to nodes on the right. In the Keras library that we'll be using, these are called <strong>dense</strong> layers.</p>
<p>For more information about the math involved, see <a target="_blank" href="http://www.deeplearningbook.org/contents/mlp.html">http://www.deeplearningbook.org/contents/mlp.html</a></p>
<h2>Convolutional Neural Networks</h2>
<p>Convolutional Neural Networks (CNNs) are a neural network configuration that is state of the art for vision and image problems and increasingly in use for natural language processing. The two main advantages are
<ul>
<li>they allow the network to take the structure of the input into account. For instance, you want the model to know that your image is two dimensional and has three color levels at each pixel.</li>
<li>they can handle some translation and rotation. That is, if a feature in a CNN has learned to recognize an eye, it can recognize it in many locations throughtout the image, perhaps even if the animal has its head at an angle.</li>
</ul>
<h3>Convolutional Layer Structure</h3>
</p>
<p>Rather than connecting every input to each neuron in the first layer, each neuron is connected to a small group of inputs that are near one another in the overall input. For an image, that would mean a square of pixels, whereas for a document it would mean a few consecutive words of a document. This allows the network to learn the two-dimensional structure of an image, the one-dimensional structure of text, the three-dimensional structure of physical space, or other structured inputs. For this, we need to set the hyperparameter of the shape of the <strong>kernel</strong> or group of inputs for each neuron (e.g. 5x5 or 3x3 pixels for image data or three word groups for text input).</p>
<p> We also need to decide how big the <strong>stride</strong> should be, or how much we shift between each group of inputs. The image below shows a stride of 2 and a 3x3 kernel. The first filter would be connected to pixels (0,0), (0,1), (0,2),(1,0), (1,1), (1,2), (2,0), (2,1), and (2,2) and the second filter to (2,0), (2,1), (2,2),(3,0), (3,1), (3,2), (4,0), (4,1), and (4,2). Both neurons are applied to the column of pixels from (2,0) to (2,2). However, the 3x3 box of pixels starting at (1,0) and ending with (3,2) does not get examined as its own group.</p>
<div class="center"><img src="assets/img/convolutional_diagram_basic.png" alt="diagram of pixels that are part of the input to a neuron" width="191"><img src="assets/img/convolutional_diagram_with_neurons.png" alt="diagram of pixels connected to neurons" width="146"></div>
<p>The outputs of the neurons are calculated just as in the feedforward network before, by summing up the weights multiplied by the corresponding inputs and applying an activation function (currently, ReLU is the most popular) to that sum. The output will be higher or lower depending on the pixel levels and the weights. Maybe one neuron gets higher outputs if the top row of pixels is darker and the rest are lighter while another most strongly activates if there are high levels of red and low levels of green and blue in the center and high levels of all colors around the edges.</p>
<p>This connection pattern helps the model use the structure in the data. If we just stopped there, we'd have a <strong>locally-connected</strong> layer, and it would have a ton of weights to learn. It would have to learn to recognize objects separately in every part of the image. One filter might be responsible for recognizing a horizontal line at the very top right corner, with another filter recognizing a horizontal line at the top and a few pixels in from the right. This would be very redundant.</p>
<p>The second important idea in a convolutional layer is <strong>parameter sharing</strong>. Instead of having the neuron connected to only one kernel worth of pixels, we use this same neuron (meaning the same weights) for each kernel position in the input. In other words, rather than learning different weights for the pink and blue neurons above, we make them learn the weights that work best overall for every group of pixels in the image. You can see an animation of this at work under the section called "Convolution Demo" on <a target="_blank" href="http://cs231n.github.io/convolutional-networks/">this page</a>.</p>
<p>We have another hyperparameter for how many of these neurons, or <strong>filters</strong> we want to have. This sets the number of different types of patterns we can recognize in this layer. A neuron can learn to activate when it sees a certain pattern anywhere in the image, because it will be applied to each group of pixels in the image.</p>
<p>The output from the convolutional layer is the output of each neuron at each position in the input that is examined, given the stride. Each filter output is essentially a processed image that shows how much each area of the image excited the neuron. In the illustration below, the layer takes 15x15 images and has a 3x3 kernel and stride of 2. It appears that filter 1 is excited if the center square of the area is red and filters 2 and 3 are excited when the center column or middle row, respectively, is blue.</p>
<div class="center"><img src="assets/img/conv_layer_outputs.png" alt="example of input image and filter outputs"></div>
<h3>Deep Convolutional Networks</h3>
<p>We can put the output of one convolutional layer into another convolutional layer. The next layer treats the output of the previous convolutional layer as though it was image with as many color channels as the previous layer had filters. For each output that comes from the kernel-sized area of the previous input, we have a level of pattern detected by the first neuron, level from the second neuron, etc. just like we might have had the levels of red, green, and blue in an image. The neurons in this level have weights for each of the channels for each of locations of their kernel size. Again, the output of each filter for each position in the input is passed on to the next layer.</p>
<p>Stacking many layers together into a deep convolutional neural network allows for powerful hierarchical processing. It appears as though the early layers learn low-level things like edge detection and simple patterns, and later layers learn more complex shapes like facial features and other compound objects. <a target="_blank" href="https://www.youtube.com/watch?v=AgkfIQ4IGaM">This video</a> gives some great examples of this, with further explanation in <a target="_blank" href="http://yosinski.com/deepvis">this article</a>.</p>
<p>The <a target="_blank" href="http://cs231n.github.io/convolutional-networks/">Stanford computer vision class</a> is a great place to go for the latest best practices with convolutional networks.</p>
<p>For more information</p>
<ul>
<li>
<a target="_blank" href="https://github.com/ageron/handson-ml/blob/master/13_convolutional_neural_networks.ipynb">https://github.com/ageron/handson-ml/blob/master/13_convolutional_neural_networks.ipynb</a> (goes along with <a target="_blank" href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/">a book</a>; I have a copy you can use in class)</li>
<li><a target="_blank" href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/">http://colah.github.io/posts/2014-07-Conv-Nets-Modular/</a></li>
<li><a target="_blank" href="http://brohrer.github.io/how_convolutional_neural_networks_work.html">http://brohrer.github.io/how_convolutional_neural_networks_work.html</a></li>
<li><a target="_blank" href="http://www.deeplearningbook.org/contents/convnets.html">http://www.deeplearningbook.org/contents/convnets.html</a></li>
<li><a target="_blank" href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/">http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/</a></li>
</ul>
<h3>Suggested Exercises</h3>
<ul>
    <li>Learn more about how the weights in a convolutional layer work, using the questions and exercises in this <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningTutorials/blob/master/Convolutional%20Neural%20Network%20Tutorial.ipynb">Jupyter Notebook.</a></li>
    <li>Get practice tuning a convolutional neural network, using this <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningTutorials/blob/master/ConvolutionalNeuralNetworkTuning.ipynb">Jupyter Notebook</a></li>
</ul>
<h2>Recurrent Neural Networks</h2>
<p>In our <a href="#fully-connected-feed-forward-network">fully connected network</a>, all information flows forward, and the inputs to a neuron are independent with respect to other inputs and to any outputs in later layers. What if instead we want to feed in a series of inputs of varying length? What if we want to also get out a sequence of varying length, called a <strong>sequence-to-sequence</strong> model (see <a href="#problems-where-neural-networks-excel">Problems Where Neural Networks Excel</a>)? We want some way to be able to let the network let information from previous inputs affect the output of the current input.</p>
<p>Recurrent Neural Networks (RNNs) allow feedback loops in the network, where the outputs from neurons can be fed back in as inputs to neurons in the same or earlier layers, and not just the immediately following layer. Below is a diagram of a simple recurrent layer with three recurrent units.</p>
<p><img src="assets/img/3_Unit_RNN_Layer.jpg" alt="recurrent neuron" width="256"></p>
<p>In the image above, each neuron has seven weighted connections, four for inputs from the previous layer and three for the previous output from each unit in the layer. In other words, at time \(t\):
    \[\begin{align}
      Out_{0,t} = & w_{0,0}x_{0,t} + w_{0,1}x_{1,t} + w_{0,2}x_{2,t} + w_{0,3}x_{3,t} \\
                  & + w_{0,4}Out_{0,t-1} + w_{0,5}Out_{1,t-1} + w_{0,6}Out_{2,t-1} \\
                = & w_{0,0}x_{0,t} + w_{0,1}x_{1,t} + w_{0,2}x_{2,t} + w_{0,3}x_{3,t} \\
                  & + w_{0,4}(w_{0,0}x_{0,t-1} + w_{0,1}x_{1,t-1} + w_{0,2}x_{2,t-1} + w_{0,3}x_{3,t-1} \\
                  & \hspace{3em}+ w_{0,4}Out_{0,t-2} + w_{0,5}Out_{1,t-2} + w_{0,6}Out_{2,t-2}) \\
                  & + w_{0,5}(w_{1,0}x_{0,t-1} + w_{1,1}x_{1,t-1} + w_{1,2}x_{2,t-1} + w_{1,3}x_{3,t-1} \\
                  & \hspace{3em}+ w_{1,4}Out_{0,t-2} + w_{1,5}Out_{1,t-2} + w_{1,6}Out_{2,t-2}) \\
                  & + w_{0,6}(w_{2,0}x_{0,t-1} + w_{2,1}x_{1,t-1} + w_{2,2}x_{2,t-1} + w_{2,3}x_{3,t-1} \\
                  & \hspace{3em}+ w_{2,4}Out_{2,t-2} + w_{2,5}Out_{1,t-2} + w_{2,6}Out_{2,t-2}) \\
    \end{align}\]
</p>

<p>And similarly each \(Out_{x,t-2}\) could be further expanded to include \(Out_{x,t-3}\) and \(Out_{x,t-4}\) and so on. \(Out_{x,t}\) is dependent on its predecessors earlier in the input sequence. \(Out_{x,0}\) is defined to be zero, to start the sequence.</p>
<h3>Long Short Term Memory</h3>
<p>A particularly important type of RNN is the <strong>Long Short Term Memory (LSTM)</strong>, which is thought to be better able to handle longer-term dependencies (that is related inputs that are further apart in the sequence) than other RNN structures. Because of this, LSTMs were the network structure of choice for natural language problems, but they appear to be falling out of favor relative to using convolutional layers as we do in our <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningTutorials/blob/master/WordVectors.ipynb">word vectors notebook.</a></p>
<p>An LSTM layer has a long-term memory unit, usually called the context or cell. This gives it a separate storage area that can be used to remember information for a longer time, without necessarily affecting the intervening outputs of the neuron that go to other layers.</p>
<p>Each LSTM layer is composed of four recurrent sub-layers that are fed the new input for a timestep, the outputs at the previous timestep, and the context state. These layers represent the context cell and three "gates" that decide what pieces of the current input and the current context get put into the next output and next context state. The output from each layer is treated slightly differently, with the goal of having the network learn roughly the following functions:</p>
<ul>
<li>Forget gate: the weights in this layer allow some long-term memories to be forgotten.</li>
<li>Input gate: the weights in this layer decide what new information will be added to the context cell.</li>
<li>Output gate: the weights in this layer decide what pieces of the new information and updated context will be passed on to the output.</li>
</ul>
<p>For more information on the details of LSTMs</p>
<ul>
<li>
<a target="_blank" href="https://github.com/ageron/handson-ml/blob/master/14_recurrent_neural_networks.ipynb">https://github.com/ageron/handson-ml/blob/master/14_recurrent_neural_networks.ipynb</a> (goes along with a book; I have a copy you can use in class)</li>
<li><a target="_blank" href="https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/">https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/</a></li>
<li>
<a target="_blank" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a><a target="_blank" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"></a>
</li>
<li><a target="_blank" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></li>
<li>
<a target="_blank" href="http://www.deeplearningbook.org/contents/rnn.html">http://www.deeplearningbook.org/contents/rnn.html</a><a target="_blank" href="http://www.deeplearningbook.org/contents/rnn.html"></a>
</li>
<li>
<a target="_blank" href="https://apaszke.github.io/lstm-explained.html">https://apaszke.github.io/lstm-explained.html</a> (code in Torch7, includes LSTM equations)</li>
</ul>
<h3>Suggested Exercises</h3>
<ul>
    <li>Complete the exercises about how recurrent neural network layers work in this <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningTutorials/blob/master/RecurrentNeuralNetworks.ipynb">Jupyter Notebook</a>.</li>
    <li>Complete one of the exercises about tuning an RNN or adapting it to a new problem in this <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningTutorials/blob/master/RecurrentNeuralNetworksTuning.ipynb">Jupyter Notebook</a>.</li>
</ul>
