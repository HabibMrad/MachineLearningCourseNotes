<h2>Neural Network Configurations</h2>
<p>There are <a target="_blank" href="http://www.asimovinstitute.org/neural-network-zoo/">many types of neural network configurations</a>, but we'd like to focus on just a few main types.</p>
<h2>Fully-Connected Feed-Forward Network</h2>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz41.png" alt="fully connected feedforward network" width="488" height="243"></p>
<p>(image from <a target="_blank" href="http://neuralnetworksanddeeplearning.com/chap6.html">http://neuralnetworksanddeeplearning.com/chap6.html</a>)</p>
<p>This is the type of network we <a href="#neural-networks">first talked about</a>. Every node is connected to every other node in the layers next to it. Each connection has its own separate weight. Information always flows forward, from nodes on the left to nodes on the right.</p>
<p>For more information about the math involved, see <a target="_blank" href="http://www.deeplearningbook.org/contents/mlp.html">http://www.deeplearningbook.org/contents/mlp.html</a></p>
<h2>Convolutional Neural Networks</h2>
<p>The main thing to know about Convolutional Neural Networks (CNNs) is that they are great for vision and image problems because</p>
<ul>
<li>they allow the network to take the structure into account. For instance, an image is two dimensional and has one, three, or four color levels at each pixel.</li>
<li>they are robust to translation and rotation. That is, if a feature in a CNN has learned to recognize an eye, it can learn to recognize it anywhere in the image, even if the animal has its head at an angle.</li>
</ul>
<p>The nodes in a convolutional layer (often called features), are connected to a small group of inputs that are near one another in the input. For instance, the first node might be connected to each of the first five pixels in the first five rows of an image (25 pixels in all), with a different weight for each pixel in this group. This allows the network to use the two-dimensional structure. For this, we need to set the hyperparameters of how big the <strong>kernel</strong> or group of inputs for each node is (e.g. 5x5 pixels, 3x3 pixels, etc.) and how big the <strong>stride</strong> should be, or how much we shift between each group. With a stride of 2 and a 3x3 kernel, the first neuron gets pixels (0,0), (0,1), (0,2),(1,0), (1,1), (1,2), (2,0), (2,1), and (2,2) while the second neuron gets (2,0), (2,1), (2,2),(3,0), (3,1), (3,2), (4,0), (4,1), and (4,2). The 3x3 box of pixels starting at (1,0) does not get examined.</p>
<p><img src="assets/img/convolutional_diagram_basic.png" alt="diagram of pixels that are part of the input to a neuron" width="191"><img src="assets/img/convolutional_diagram_with_neurons.png" alt="diagram of pixels connected to neurons" width="146"></p>
<p>That gets us the structure from our data, and if we just stopped there, we'd have a <strong>locally-connected</strong> layer, and it would have a ton of weights to learn. It would have to learn to recognize objects separately in every part of the image -- very redundant.</p>
<p>The second important part is <strong>parameter sharing</strong>. Instead of having the neuron connected to only one kernel worth of pixels, we use this same neuron (meaning the same weights) for each kernel position in the input. In other words, rather than learning different weights for the pink and blue neurons above, we make them share weights, learning the weights that work best overall for every group of pixels in the image. Now, our hyperparameter is how many of these neurons, or <strong>features</strong> we want to have. A neuron can learn to activate when it sees a certain pattern anywhere in the image, because it will be applied to each group of pixels.</p>
<p>Here are parts of two convolutional layers each with a 2x1 kernel and stride of 1, from <a target="_blank" href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/">this article about CNNs</a>. The two neurons, A and B, each have two weights, one for the left input of a pair and one for the right input of a pair. These weights are applied to every single input pair, with overlap. You can see an animation of this at work under the section called "Convolution Demo" on <a target="_blank" href="http://cs231n.github.io/convolutional-networks/">this page</a>.</p>
<p><img src="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/img/Conv-9-Conv2Conv2.png" alt="2 convolutional layers with 3x1 kernel" width="387" height="170"></p>
<p>As you can read about in the resources below, in a deep convolutional neural network, it appears as though the early layers learn low-level things like edge detection and simple patterns, and later layers learn more complex shapes like facial features and other compound objects. <a target="_blank" href="https://www.youtube.com/watch?v=AgkfIQ4IGaM">This video</a> gives some great examples of this, with further explanation in <a target="_blank" href="http://yosinski.com/deepvis">this article</a>.</p>
<p>For more information</p>
<ul>
<li>The Stanford class on CNNs is a great place to go for the latest best practices: <a target="_blank" href="http://cs231n.github.io/convolutional-networks/">http://cs231n.github.io/convolutional-networks/</a></li>
<li>
<a target="_blank" href="https://github.com/ageron/handson-ml/blob/master/13_convolutional_neural_networks.ipynb">https://github.com/ageron/handson-ml/blob/master/13_convolutional_neural_networks.ipynb</a> (goes along with a book; I have a copy you can use in class)</li>
<li><a target="_blank" href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/">http://colah.github.io/posts/2014-07-Conv-Nets-Modular/</a></li>
<li><a target="_blank" href="http://brohrer.github.io/how_convolutional_neural_networks_work.html">http://brohrer.github.io/how_convolutional_neural_networks_work.html</a></li>
<li><a target="_blank" href="http://www.deeplearningbook.org/contents/convnets.html">http://www.deeplearningbook.org/contents/convnets.html</a></li>
<li><a target="_blank" href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/">http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/</a></li>
</ul>
<p>To better understand the inner workings of convolutional neural networks, check out our <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningTutorials/blob/master/Convolutional%20Neural%20Network%20Tutorial.ipynb">Jupyter Notebook tutorial.</a></p>
<h2>Recurrent Neural Networks</h2>
<p>In our <a href="#fully-connected-feed-forward-network">fully connected network</a>, all information flows forward, and the inputs to a neuron are independent with respect to other inputs and to any outputs in later layers. What if we want a sequence to sequence network, where we feed in pieces of an input one at a time (see <a href="#problems-where-neural-networks-excel">Problems Where Neural Networks Excel</a>)?</p>
<p>Recurrent Neural Networks (RNNs) allow feedback loops in the network, where the outputs from neurons can be fed back in as inputs to neurons in the same or earlier layers, and not just the immediately following layer.</p>
<p><img src="assets/img/3_Unit_RNN_Layer.jpg" alt="recurrent neuron" width="256"></p>
<p>In the image above, each neuron has seven weighted connections, four for inputs from the previous layer and three for the previous output from each unit in the layer. In other words, at time \(t\):
    \[\begin{align}
      Out_{0,t} = & w_{0,0}x_{0,t} + w_{0,1}x_{1,t} + w_{0,2}x_{2,t} + w_{0,3}x_{3,t} \\
                  & + w_{0,4}Out_{0,t-1} + w_{0,5}Out_{1,t-1} + w_{0,6}Out_{2,t-1} \\
                = & w_{0,0}x_{0,t} + w_{0,1}x_{1,t} + w_{0,2}x_{2,t} + w_{0,3}x_{3,t} \\
                  & + w_{0,4}(w_{0,0}x_{0,t-1} + w_{0,1}x_{1,t-1} + w_{0,2}x_{2,t-1} + w_{0,3}x_{3,t-1} \\
                  & \hspace{3em}+ w_{0,4}Out_{0,t-2} + w_{0,5}Out_{1,t-2} + w_{0,6}Out_{2,t-2}) \\
                  & + w_{0,5}(w_{1,0}x_{0,t-1} + w_{1,1}x_{1,t-1} + w_{1,2}x_{2,t-1} + w_{1,3}x_{3,t-1} \\
                  & \hspace{3em}+ w_{1,4}Out_{0,t-2} + w_{1,5}Out_{1,t-2} + w_{1,6}Out_{2,t-2}) \\
                  & + w_{0,6}(w_{2,0}x_{0,t-1} + w_{2,1}x_{1,t-1} + w_{2,2}x_{2,t-1} + w_{2,3}x_{3,t-1} \\
                  & \hspace{3em}+ w_{2,4}Out_{2,t-2} + w_{2,5}Out_{1,t-2} + w_{2,6}Out_{2,t-2}) \\
    \end{align}\]
</p>

<p>And similarly each \(Out_{x,t-2}\) could be further expanded to include \(Out_{x,t-3}\) and \(Out_{x,t-4}\) and so on. \(Out_{x,t}\) is dependent on its predecessors earlier in the input sequence.</p>
<p>A particularly important type of RNN is the <strong>Long Short Term Memory (LSTM)</strong>, which thought to be better able to handle longer-term dependencies (that is related inputs that are further apart in the sequence) than other RNN structures. An LSTM unit, in addition to the usual output that is passed on to later layers and fed back into itself, has a long-term memory unit, usually called the context or cell. This gives it a separate storage area that can be used to remember information for a longer time, without necessarily affecting the intervening outputs of the neuron that go to other layers.</p>
<p>The LSTM is composed of four fully connected neural networks that are fed both the new input for a timestep and the output of the previous timestep. The outputs of these networks are passed through three "gates" to decide what pieces of the current input and the context get put into the next output and context.</p>
<ul>
<li>Forget gate: these weights allow some long-term memories to be forgotten.</li>
<li>Input gate: these weights decide what new information will be added to the context cell.</li>
<li>Output gate: these weights decide what pieces of the new information and updated context will be passed on to the output.</li>
</ul>
<p>See this <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningTutorials/blob/master/RecurrentNeuralNetworks.ipynb">Jupyter Notebook</a> for some code examples and exercises.</p>
<p>For more information</p>
<ul>
<li>
<a target="_blank" href="https://github.com/ageron/handson-ml/blob/master/14_recurrent_neural_networks.ipynb">https://github.com/ageron/handson-ml/blob/master/14_recurrent_neural_networks.ipynb</a> (goes along with a book; I have a copy you can use in class)</li>
<li><a target="_blank" href="https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/">https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/</a></li>
<li>
<a target="_blank" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a><a target="_blank" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"></a>
</li>
<li><a target="_blank" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></li>
<li>
<a target="_blank" href="http://www.deeplearningbook.org/contents/rnn.html">http://www.deeplearningbook.org/contents/rnn.html</a><a target="_blank" href="http://www.deeplearningbook.org/contents/rnn.html"></a>
</li>
<li>
<a target="_blank" href="https://apaszke.github.io/lstm-explained.html">https://apaszke.github.io/lstm-explained.html</a> (code in Torch7, includes LSTM equations)</li>
</ul>
