<h2>Neural Network Configurations</h2>
<p>There are <a target="_blank" href="http://www.asimovinstitute.org/neural-network-zoo/">many types of neural network configurations</a>, but we'd like to focus on just a few main types.</p>
<h2>Fully-Connected Feed-Forward Network</h2>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz41.png" alt="fully connected feedforward network" width="488" height="243"></p>
<p>(image from <a target="_blank" href="http://neuralnetworksanddeeplearning.com/chap6.html">http://neuralnetworksanddeeplearning.com/chap6.html</a>)</p>
<p>This is the type of network we <a href="#neural-networks">first talked about</a>. Every node is connected to every other node in the layers next to it. Each connection has its own separate weight. Information always flows forward, from nodes on the left to nodes on the right.</p>
<p>For more information about the math involved, see <a target="_blank" href="http://www.deeplearningbook.org/contents/mlp.html">http://www.deeplearningbook.org/contents/mlp.html</a></p>
<h2>Convolutional Neural Networks</h2>
<p>The main thing to know about Convolutional Neural Networks (CNNs) is that they are great for vision and image problems because</p>
<ul>
<li>they allow the network to take the structure into account. For instance, an image is two dimensional and has one, three, or four color levels at each pixel.</li>
<li>they are robust to translation and rotation. That is, if a feature in a CNN has learned to recognize an eye, it can learn to recognize it anywhere in the image, even if the animal has its head at an angle.</li>
</ul>
<p>The nodes in a convolutional layer (often called filters or features), are connected to a small group of inputs that are near one another. For instance, the first node might be connected to each of the first five pixels in the first five rows of an image (25 pixels in all), with a different weight for each pixel in this group. This allows the network to learn the two-dimensional structure of an image, the one-dimensional structure of text, the three-dimensional structure of physical space, or other structured inputs. For this, we need to set the hyperparameters of how big the <strong>kernel</strong> or group of inputs for each node is (e.g. 5x5 pixels or 3x3 pixels for image data).</p>
<p> We also need to decide how big the <strong>stride</strong> should be, or how much we shift between each group of inputs. With a stride of 2 and a 3x3 kernel, the first neuron gets pixels (0,0), (0,1), (0,2),(1,0), (1,1), (1,2), (2,0), (2,1), and (2,2) while the second neuron gets (2,0), (2,1), (2,2),(3,0), (3,1), (3,2), (4,0), (4,1), and (4,2). The 3x3 box of pixels starting at (1,0) does not get examined.</p>
<p><img src="assets/img/convolutional_diagram_basic.png" alt="diagram of pixels that are part of the input to a neuron" width="191"><img src="assets/img/convolutional_diagram_with_neurons.png" alt="diagram of pixels connected to neurons" width="146"></p>
<p>That gets us the structure from our data, and if we just stopped there, we'd have a <strong>locally-connected</strong> layer, and it would have a ton of weights to learn. It would have to learn to recognize objects separately in every part of the image -- very redundant.</p>
<p>The second important part is <strong>parameter sharing</strong>. Instead of having the neuron connected to only one kernel worth of pixels, we use this same neuron (meaning the same weights) for each kernel position in the input. In other words, rather than learning different weights for the pink and blue neurons above, we make them share weights, learning the weights that work best overall for every group of pixels in the image. Now, our hyperparameter is how many of these neurons, or <strong>filters</strong> we want to have. A neuron can learn to activate when it sees a certain pattern anywhere in the image, because it will be applied to each group of pixels in the image.</p>
<p>Here are parts of two convolutional layers each with a 2x1 kernel and stride of 1, from <a target="_blank" href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/">this article about CNNs</a>. The two neurons, A and B, each have two weights, one for the left input of a pair and one for the right input of a pair. These weights are applied to every single input pair, with overlap. You can see an animation of this at work under the section called "Convolution Demo" on <a target="_blank" href="http://cs231n.github.io/convolutional-networks/">this page</a>.</p>
<p><img src="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/img/Conv-9-Conv2Conv2.png" alt="2 convolutional layers with 3x1 kernel" width="387" height="170"></p>
<p>As you can read about in the resources below, in a deep convolutional neural network, it appears as though the early layers learn low-level things like edge detection and simple patterns, and later layers learn more complex shapes like facial features and other compound objects. <a target="_blank" href="https://www.youtube.com/watch?v=AgkfIQ4IGaM">This video</a> gives some great examples of this, with further explanation in <a target="_blank" href="http://yosinski.com/deepvis">this article</a>.</p>
<p>For more information</p>
<ul>
<li>The Stanford class on CNNs is a great place to go for the latest best practices: <a target="_blank" href="http://cs231n.github.io/convolutional-networks/">http://cs231n.github.io/convolutional-networks/</a></li>
<li>
<a target="_blank" href="https://github.com/ageron/handson-ml/blob/master/13_convolutional_neural_networks.ipynb">https://github.com/ageron/handson-ml/blob/master/13_convolutional_neural_networks.ipynb</a> (goes along with <a target="_blank" href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/">a book</a>; I have a copy you can use in class)</li>
<li><a target="_blank" href="http://colah.github.io/posts/2014-07-Conv-Nets-Modular/">http://colah.github.io/posts/2014-07-Conv-Nets-Modular/</a></li>
<li><a target="_blank" href="http://brohrer.github.io/how_convolutional_neural_networks_work.html">http://brohrer.github.io/how_convolutional_neural_networks_work.html</a></li>
<li><a target="_blank" href="http://www.deeplearningbook.org/contents/convnets.html">http://www.deeplearningbook.org/contents/convnets.html</a></li>
<li><a target="_blank" href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/">http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/</a></li>
</ul>
<h3>Suggested Exercises</h3>
<ul>
    <li>Learn more about how the weights in a convolutional layer work, using the questions and exercises in this <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningTutorials/blob/master/Convolutional%20Neural%20Network%20Tutorial.ipynb">Jupyter Notebook.</a></li>
    <li>Get practice tuning a convolutional neural network, using this <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningTutorials/blob/master/ConvolutionalNeuralNetworkTuning.ipynb">Jupyter Notebook</a></li>
</ul>
<h2>Recurrent Neural Networks</h2>
<p>In our <a href="#fully-connected-feed-forward-network">fully connected network</a>, all information flows forward, and the inputs to a neuron are independent with respect to other inputs and to any outputs in later layers. What if instead we want to feed in a series of inputs of varying length? What if we want to also get out a sequence of varying length, called a <strong>sequence-to-sequence</strong> model (see <a href="#problems-where-neural-networks-excel">Problems Where Neural Networks Excel</a>)? We want some way to be able to let the network let information from previous inputs affect the output of the current input.</p>
<p>Recurrent Neural Networks (RNNs) allow feedback loops in the network, where the outputs from neurons can be fed back in as inputs to neurons in the same or earlier layers, and not just the immediately following layer. Below is a diagram of a simple recurrent layer with three recurrent units.</p>
<p><img src="assets/img/3_Unit_RNN_Layer.jpg" alt="recurrent neuron" width="256"></p>
<p>In the image above, each neuron has seven weighted connections, four for inputs from the previous layer and three for the previous output from each unit in the layer. In other words, at time \(t\):
    \[\begin{align}
      Out_{0,t} = & w_{0,0}x_{0,t} + w_{0,1}x_{1,t} + w_{0,2}x_{2,t} + w_{0,3}x_{3,t} \\
                  & + w_{0,4}Out_{0,t-1} + w_{0,5}Out_{1,t-1} + w_{0,6}Out_{2,t-1} \\
                = & w_{0,0}x_{0,t} + w_{0,1}x_{1,t} + w_{0,2}x_{2,t} + w_{0,3}x_{3,t} \\
                  & + w_{0,4}(w_{0,0}x_{0,t-1} + w_{0,1}x_{1,t-1} + w_{0,2}x_{2,t-1} + w_{0,3}x_{3,t-1} \\
                  & \hspace{3em}+ w_{0,4}Out_{0,t-2} + w_{0,5}Out_{1,t-2} + w_{0,6}Out_{2,t-2}) \\
                  & + w_{0,5}(w_{1,0}x_{0,t-1} + w_{1,1}x_{1,t-1} + w_{1,2}x_{2,t-1} + w_{1,3}x_{3,t-1} \\
                  & \hspace{3em}+ w_{1,4}Out_{0,t-2} + w_{1,5}Out_{1,t-2} + w_{1,6}Out_{2,t-2}) \\
                  & + w_{0,6}(w_{2,0}x_{0,t-1} + w_{2,1}x_{1,t-1} + w_{2,2}x_{2,t-1} + w_{2,3}x_{3,t-1} \\
                  & \hspace{3em}+ w_{2,4}Out_{2,t-2} + w_{2,5}Out_{1,t-2} + w_{2,6}Out_{2,t-2}) \\
    \end{align}\]
</p>

<p>And similarly each \(Out_{x,t-2}\) could be further expanded to include \(Out_{x,t-3}\) and \(Out_{x,t-4}\) and so on. \(Out_{x,t}\) is dependent on its predecessors earlier in the input sequence. \(Out_{x,0}\) is defined to be zero, to start the sequence.</p>
<h3>Long Short Term Memory</h3>
<p>A particularly important type of RNN is the <strong>Long Short Term Memory (LSTM)</strong>, which is thought to be better able to handle longer-term dependencies (that is related inputs that are further apart in the sequence) than other RNN structures. Because of this, LSTMs were the network structure of choice for natural language problems, but they appear to be falling out of favor relative to using convolutional layers as we do in our <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningTutorials/blob/master/WordVectors.ipynb">word vectors notebook.</a></p>
<p>An LSTM layer has a long-term memory unit, usually called the context or cell. This gives it a separate storage area that can be used to remember information for a longer time, without necessarily affecting the intervening outputs of the neuron that go to other layers.</p>
<p>Each LSTM layer is composed of four recurrent sub-layers that are fed the new input for a timestep, the outputs at the previous timestep, and the context state. These layers represent the context cell and three "gates" that decide what pieces of the current input and the current context get put into the next output and next context state. The output from each layer is treated slightly differently, with the goal of having the network learn roughly the following functions:</p>
<ul>
<li>Forget gate: the weights in this layer allow some long-term memories to be forgotten.</li>
<li>Input gate: the weights in this layer decide what new information will be added to the context cell.</li>
<li>Output gate: the weights in this layer decide what pieces of the new information and updated context will be passed on to the output.</li>
</ul>
<p>For more information on the details of LSTMs</p>
<ul>
<li>
<a target="_blank" href="https://github.com/ageron/handson-ml/blob/master/14_recurrent_neural_networks.ipynb">https://github.com/ageron/handson-ml/blob/master/14_recurrent_neural_networks.ipynb</a> (goes along with a book; I have a copy you can use in class)</li>
<li><a target="_blank" href="https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/">https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/</a></li>
<li>
<a target="_blank" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a><a target="_blank" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"></a>
</li>
<li><a target="_blank" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></li>
<li>
<a target="_blank" href="http://www.deeplearningbook.org/contents/rnn.html">http://www.deeplearningbook.org/contents/rnn.html</a><a target="_blank" href="http://www.deeplearningbook.org/contents/rnn.html"></a>
</li>
<li>
<a target="_blank" href="https://apaszke.github.io/lstm-explained.html">https://apaszke.github.io/lstm-explained.html</a> (code in Torch7, includes LSTM equations)</li>
</ul>
<h3>Suggested Exercises</h3>
<ul>
    <li>Complete the exercises about how recurrent neural network layers work in this <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningTutorials/blob/master/RecurrentNeuralNetworks.ipynb">Jupyter Notebook</a>.</li>
    <li>Complete one of the exercises about tuning an RNN or adapting it to a new problem in this <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningTutorials/blob/master/RecurrentNeuralNetworksTuning.ipynb">Jupyter Notebook</a>.</li>
</ul>
