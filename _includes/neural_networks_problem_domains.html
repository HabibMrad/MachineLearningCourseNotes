<h2>Problems Where Neural Networks Excel</h2>
<h3>Computer Vision</h3>
<p>Computer vision was the first area in which neural networks became state of the art. Initially, these were largely classification problems like <a target="_blank" href="http://yann.lecun.com/exdb/mnist/">MNIST</a> and <a target="_blank" href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR</a>, where the network had to decide what the image was from a limited number of possible answers. Now, they are being used for more complicated problems, such as <a target="_blank" href="https://github.com/jcjohnson/neural-style">combining images</a>, <a target="_blank" href="http://cs.stanford.edu/people/karpathy/deepimagesent/">identifying multiple items in an image</a>, or <a target="_blank" href="https://openai.com/blog/generative-models/">generating new images</a>.</p>
<h3>Sequence to Sequence:</h3>
<p>These are networks that aren't looking to produce one set of outputs for each set of inputs, but instead take a sequence of inputs and produce a sequence of outputs. For instance, when <a target="_blank" href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html">translating from one language to another</a>, you don't want to do a word-for-word translation, because you want to use the context of the other words. But if you put in entire sentences (or paragraphs or essays!) you would need a huge number of input nodes. You would also have to set some fixed limit on the longest passage you could translate, and the fact that the output might be a very different size from the input (for instance, if one language has a word that another language needs several words to replace) might give you problems. Instead, we feed in individual characters or words (perhaps as embedded vectors) and have the network remember and use what it has already seen. A similar problem arises in <a target="_blank" href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">translating between speech and text</a>.</p>
<h3>Manifold learning:</h3>
<p>Some neural networks are attempting to simply reproduce their inputs. By making some of the layers small enough that they are unable to copy the input through, they have to learn a more compact representation of the input. The idea is that there is some manifold -- a lower dimensional area -- on or around which all good inputs lie within the much higher dimensional space of all possible inputs. For instance, if you were to pick random letters from the English alphabet, you would only sometimes create an actual English word. We want to learn if there is some pattern that makes English words look different from strings of letters that are not English words. These types of networks can be used to compress data or find better ways to represent data before using it as input into another neural network. These types of models can also be used to remove noise or errors from inputs, by looking for a known good input that is "close" to the current input. Finally, they can be used as <strong>generators</strong>, producing similar outputs to what had been seen before.</p>
