<h2>Training a Neural Network</h2>
<p>In order to learn the weights that connections between neurons in the network should have, we need to <strong>backpropagate</strong> the information about how far off our desired output was from our actual output. This is often done using a technique called <strong>stochastic gradient descent (SGD).</strong></p>
<p>First, we need a <strong>cost function</strong> or <strong>loss function</strong> that allows us to measure how how different the desired and actual output are. We've seen this before with regression, where we were looking to minimize the sum of the squared errors. Frequently, we use the <strong>cross entropy</strong> a measure of the difference in information between the distributions of the model outputs and the desired outputs. (see <a target="_blank" href="https://jamesmccaffrey.wordpress.com/2013/11/05/why-you-should-use-cross-entropy-error-instead-of-classification-error-or-mean-squared-error-for-neural-network-classifier-training/">here</a>).</p>
<p>Now, we can take a look at the <a target="_blank" href="http://mathinsight.org/directional_derivative_gradient_introduction"><strong>gradient</strong></a> of our cost function. The gradient tells us how the cost function is changing at any particular point. For any particular feature, will I get a better score if I move its weight up or down? Because both neural networks and datasets tend to be very large, doing the entire optimization all at once is prohibitive. Instead, we take in <strong>mini-batches</strong> of the data (perhaps even just one at a time, depending on the amount of memory you have available) and and adjust each weight based on the gradient of the cost function for this batch. This is SGD (<strong>stochastic</strong> refers to a random process, which this is because we are taking small pieces of our dataset in a random order). The larger the mini-batch, the closer we will be to the actual gradient of the cost function for the whole dataset when adjusting weights. However, a larger mini-batch also means more memory and computation per batch.</p>
<p>For each batch, we first do a forward pass, running the input through the network. (See <a target="_blank" href="http://techeffigytutorials.blogspot.com/2015/01/neural-network-illustrated-step-by-step.html">these illustrations</a> or <a target="_blank" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">these illustrations</a>.) Next, we look at the differences between the final output and what we wanted. For each output neuron, we look at the connections coming in, to see how much they contributed to the error. These go to the previous layer, so now we know what we wanted that to be, and we can move back one layer and repeat the process to adjust those weights. In this way, we <strong>backpropagate</strong> the error, adjusting the weights to make it smaller.</p>
<p>Backpropagating one batch of the data is called an <strong>iteration. </strong>Going all the way through the training set is called an <strong>epoch</strong>. Often, training libraries will run a validation pass on the test set after each epoch.</p>
<p>In a neural network, each weight for each connection between neurons is a <strong>parameter</strong> that must be learned through training. There are also a number of unlearned values that we choose ahead of time, called <a href="#parameters-and-hyperparameters">hyperparameters</a>. Examples of these are the number of layers in the network, the number of neurons per layer, the <strong>learning rate</strong> which dictates how far along the <a target="_blank" href="http://mathinsight.org/directional_derivative_gradient_introduction">gradient</a> we move in each step, and the activation function in each neuron or layer. You could also use <a target="_blank" href="https://cs231n.github.io/neural-networks-3/#ada">more complex backpropagation algorithms than standard SGD</a>, such as <a target="_blank" href="https://arxiv.org/pdf/1412.6980.pdf">Adam</a> or <a target="_blank" href="https://arxiv.org/abs/1212.5701">ADADELTA</a> which use adaptive learning rates. You can attempt to learn good hyperparameter settings by picking <a target="_blank" href="https://cs231n.github.io/neural-networks-3/#hyper">random combinations of values</a> and using cross validation to choose the best-performing values.</p>
<p>Because neural networks are so large and have so many parameters to fit, there is a huge danger of overfitting to your training data and being unable to generalize to new examples. <strong>Regularization </strong>methods to try to deal with this problem include:</p>
<ul>
<li>early stopping: don't train your network all the way to a minimum value of the cost function, which is likely to be specific to your training data. Instead, once you notice that you are no longer getting much better, return to the earlier model around the point where the slowdown started (it's like the <a target="_blank" href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)">elbow method</a> again!)</li>
<li>
<a target="_blank" href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">dropout layers</a>: randomly omit some neurons during steps of the training process -- similar to what <a href="#random-forest">Random Forest</a> does when creating its decision trees</li>
<li>sparse layers: set the smallest weights to zero, so that you have fewer connections that actually do anything</li>
<li>L1 or L2 regularization (what we saw with regression): add to the cost function a penalty of the sum of the absolute value (L1) or square (L2) of the weights</li>
<li>batch normalization: normalize the weights (that is, subtract the mean and divide by the standard deviation) that are learned for a particular batch of data at each layer of the network</li>
</ul>
<p>Another important thing to consider is <strong>weight initialization</strong>. It turns out that the weights your connections have when you start training can have a huge effect on how well you can train your network. The general recommendation is to use <a target="_blank" href="http://cs231n.github.io/neural-networks-2/#init">random numbers near zero</a>. If you set them to exactly zero, all gradients would look the same for any neuron connected to the same inputs, so it would be impossible for them to learn different aspects of the data.</p>
<h3>Videos:</h3>
<ul>
    <li><a target="_blank" href="http://bit.ly/2fMZljF">But What *is* a Neural Network?</a></li>
    <li><a target="_blank" href="http://bit.ly/2zFHx6e">Gradient Descent, How Neural Networks Learn</a></li>
    <li><a target="_blank" href="http://bit.ly/2zxahhj">What is Backpropagation and What is it Actually Doing?</a></li>
</ul>
<h3>Further reading:</h3>
<ul>
<li>
<a target="_blank" href="http://techeffigytutorials.blogspot.com/2015/01/neural-network-illustrated-step-by-step.html">http://techeffigytutorials.blogspot.com/2015/01/neural-network-illustrated-step-by-step.html</a> (step-by-step diagrams of forward and backward propagation)</li>
<li>
<a target="_blank" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/</a> (diagrams, gives the math equations as well as an example with actual numbers)</li>
<li>
<a target="_blank" href="http://iamtrask.github.io/2015/07/12/basic-python-network/">http://iamtrask.github.io/2015/07/12/basic-python-network/</a> (fully explains the code for a very simple neural network)</li>
<li>
<a target="_blank" href="https://cs231n.github.io/optimization-1/">https://cs231n.github.io/optimization-1/</a> (somewhat math heavy, includes code)</li>
<li>
<a target="_blank" href="http://neuralnetworksanddeeplearning.com/chap2.html">http://neuralnetworksanddeeplearning.com/chap2.html</a> (math heavy, but also includes code in the explanation)<a target="_blank" href="http://neuralnetworksanddeeplearning.com/chap2.html"></a>
</li>
<li>
<a target="_blank" href="http://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/">http://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/</a> (less math heavy and includes code)</li>
<li>
<a target="_blank" href="http://www.deeplearningbook.org/contents/mlp.html">http://www.deeplearningbook.org/contents/mlp.html</a> (math heavy)</li>
<li>
<a target="_blank" href="http://www.pyimagesearch.com/2016/09/26/a-simple-neural-network-with-python-and-keras/">http://www.pyimagesearch.com/2016/09/26/a-simple-neural-network-with-python-and-keras/</a> explains how to create and train a network using the Keras library</li>
</ul>
