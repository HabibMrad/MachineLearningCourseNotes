<h2>Policy Gradient</h2>
<p>When using <strong>policy gradient</strong> to train a reinforcement learning model, we have a neural network that acts a <strong>policy</strong> network. This model gives as output the probability that any particular action should be taken.</p>

<p>To get training data and make this look more like a supervised learning problem, we perform a <strong>roll-out</strong> (or <strong>episode</strong>): we keep taking actions (choosing randomly, weighted by the probability outputs of the model) and updating the state until we hit a terminal condition and get a reward. After we have a bunch of these roll-outs, we use them to perform backpropagation to update the weights for every decision made in a roll-out. Thus, all actions taken in a roll-out that later gets a positive reward get encouraged (positive gradient) and all taken in a roll-out with a negative outcome get discouraged (negative gradient). That basic idea works surprisingly well, but for larger problems, you may need to use some tricks or optimizations such as <a target="_blank" href="https://arxiv.org/abs/1502.05477">Trust Region Policy Optimization</a>.</p>

<p>There are a number of ways to give a score to each action in the rollout, but the one way is to take each state/action pair and set its target value to be the sum of all of the rewards. Each reward is multiplied by a discount factor, raised to the number of steps left in that episode until the reward was reached. For any actions that were not taken from a particular state, their target can be the model's current estimate.</p>
