<h2>Regression Validation</h2>
<h3>R<sup>2</sup></h3>
<p class="indented">
    $$ \frac{1 - sum( (y_predicted - y_truth)^2)}{sum( (averageY - y_truth)^2 )} $$
</p>
<p>That is, the numerator is our usual sum of squared errors, and the denominator is the sum of squares of the difference between each point and the average y value. (<a target="_blank" href="https://www.graphpad.com/guides/prism/6/curve-fitting/index.htm?r2_ameasureofgoodness_of_fitoflinearregression.htm">See
    here for more information</a>.) An \(R^2\) of 1 means the model perfectly predicts the output. R<sup>2</sup> of 0 means the model is not any more predictive than just guessing the average output every time. A negative R<sup>2</sup> means
  your model does worse than just using the average value.</p>
<p>However, as <a target="_blank" href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet">Anscombe's Quartet</a> famously shows, you may have a high R<sup>2</sup> even if you have severely mismodeled your data.</p>
<h3>Root Mean Squared Error (RMSE)</h3>
<p>The RMSE is the square root of the following equation:</p>
<p class="indented">
  $$ \frac{sum((y_predicted - y_truth)^2)}{numDatapoints} $$
</p>
<p>This measurement represents the standard deviation of the residuals. (<strong>Residuals</strong> are equal to the difference between your prediction and the actual value of what you're trying to predict.)</p>
<p>The lower the RMSE of your model, the better it is.</p>
<p>You can also visually assess how well your model fit by plotting your residuals. If your regression model was a useful one, then the residuals should appear to be randomly distributed around the X-axis, and their average should be approximately
  zero.</p>
