<h2>Reinforcement Learning</h2>
<p><strong>Reinforcement learning</strong> is somewhere in between supervised and unsupervised learning. It is meant to work with problems where we have an overall goal we want to get to, but we need the computer to learn lots of intermediate steps to get there, and we are not sure what those intermediate steps are.</p>
<p>As input, we give our model a representation of the current state of a system and have it choose an action as output. We compute the next state of the system given that output and give the model a reward based on that new state. That is, we don't tell the model what answer we want (as we do in supervised learning), but we do tell it how much we liked its answer. It uses this reward (positive or negative) to learn what actions to take.</p>
<p>For instance, if the model is learning to play checkers, we give it the current state of the board and have it pick among the legal moves available. If this move makes it win the game, we know that it gets a positive reward. If it makes it lose the game, we know that it gets a negative reward. Most of the time, we don't know (yet) what reward to give it. We want the model to learn what actions taken in a particular state should get credit for the reward, since it was certainly not the last action alone that did it. This is known as the <strong>credit assignment problem</strong>. </p>

<h3>Q Learning</h3>

<p>In  Q learning, the idea is that we are trying to learn a function <em>Q</em> that takes in two inputs -- the current state of the world and a particular action -- and returns the <em>quality</em> or value of taking that action from that state.</p>
<p>In terms of making this more like a supervised problem, what we want is to have our model take in the current state of the environment and give us the correct quality value for each possible action that could be taken. (Alternatively, it could take in both the state and an action and return the quality value for that action only, but below I will be assuming that the action is not an input.)</p>
<p>To gain training data, we start interacting with the environment, taking random actions and receiving rewards. As we build up (state, action, reward, new state) quadruplets, we can start using these as our training data. We randomly pick some of these as the next batch we'll show the model, a technique called <strong>experience replay</strong>. Randomly choosing the order can help the model train better, so that it is not seeing many similar states at one time, the way it would if we went chronologically.</p>
<p>The label (the thing we are telling the model it <em>should</em> have returned, for training purposes) is the reward we actually got for taking that action in the input state. Of course, not every action we take gets a reward -- that's part of what makes this a reinforcement learning problem. If this action did not terminate the episode (and we might still get further rewards), then we set the target value to be the current reward plus the value of the new state that the action got us to (at a discount, because this state should not get full credit for the next state's worth).</p>
<p>In other words, for a particular state, the targets for each action are</p>
<ul>
    <li> for every action that was not the one we took this time: the model's current value of that (state, action) pair</li>
    <li>for the action we took
        <ul>
            <li>if it ended the episode: the reward we obtained</li>
            <li>if the episode was still going: the reward we obtained + discount * max(value of each action possible from the next state)</li>
        </ul>
    </li>
</ul>
<p> After training on enough input states and values for each action from that state, the model learns to value the actions at a particular state well, meaning that the action with the maximum value is the best one to take in this state.</p>

<h3>Policy Gradient</h3>

<p>Models like neural networks can be used to learn the Q function. Since neural networks learn using gradient descent, this technique is sometimes referred to as <strong>policy gradient</strong>. Another name is <strong>Deep Q Learning</strong>, a reference to the deep hierarchical structure of multi-layer neural networks.</p>

<p>To get training data and make this look more like a supervised learning problem, we perform a <strong>roll-out</strong> (or <strong>episode</strong>): we keep taking actions (choosing randomly, weighted by the probability outputs of the model) and updating the state until we hit a terminal condition and get a reward. After we have a bunch of these roll-outs, we use them to perform backpropagation to update the weights for every decision made in a roll-out. Thus, all actions taken in a roll-out that later gets a positive reward get encouraged (positive gradient) and all taken in a roll-out with a negative outcome get discouraged (negative gradient). That basic idea works surprisingly well, but for larger problems, you may need to use some tricks or optimizations such as <a target="_blank" href="https://arxiv.org/abs/1502.05477">Trust Region Policy Optimization</a>.</p>

<p>There are a number of ways to give a score to each action in the rollout, but the one way is to take each state/action pair and set its target value to be the sum of all of the rewards. Each reward is multiplied by a discount factor, raised to the number of steps left in that episode until the reward was reached. For any actions that were not taken from a particular state, their target can be the model's current estimate.</p>

<h3>Further Resources:</h3>
<ul>
    <li><a target="_blank" href="http://kvfrans.com/reinforcement-learning-basics/">Reinforcement Learning Basics by Kevin Frans</a></li>
    <li><a target="_blank" href="https://www.youtube.com/watch?v=e3Jy2vShroE">[6 min] Intro to Reinforcement Learning</a></li>
    <li><a target="_blank" href="https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT">[1.5 hours] Intro to Reinforcement Learning</a> (There is a lot more math and technical details in this video.)</li>
    <li><a target="_blank" href="https://www.youtube.com/watch?v=79pmNdyxEGo&amp;t=9s">[9 min] Deep Q-Learning for Playing Video Games</a></li>
    <li><a target="_blank" href="https://www.youtube.com/watch?v=Ih8EfvOzBOY">[3 min] Google DeepMind's Deep Q-Learning</a></li>
    <li>Solving Cartpole (a classic reinforcement learning problem):
        <ul>
            <li><a target="_blank" href="http://kvfrans.com/simple-algoritms-for-solving-cartpole/">Simple Algorithms for Solving Cartpole </a></li>
            <li><a target="_blank" href="https://keon.io/deep-q-learning/">Deep Q-Learning with Keras and Gym</a></li>
            <li><a target="_blank" href="https://medium.com/@gtnjuvin/my-journey-into-deep-q-learning-with-keras-and-gym-3e779cc12762">A Journey Into Deep Q-Learning with Keras and Gym</a></li>
            <li><a target="_blank" href="https://github.com/carlshan/intro_to_machine_learning/blob/master/lessons/Reinforcement_Learning/RL_Tutorial.md">Carl's tutorial on solving Cartpole</a></li>
        </ul>
    </li>
    <li><a target="_blank" href="http://karpathy.github.io/2016/05/31/rl/">Deep Reinforcement Learning to Play Pong</a></li>
    <li><a target="_blank" href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/">Deep Reinforcement Learning to Play Breakout aka Brick Breaker</a></li>
    <li><a target="_blank" href="https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html">Keras Deep Q Learning to Play Flappy Bird</a></li>
    <li>Here are some additional resources recommended by a speaker who visited a previous class:
        <ul>
            <li><a target="_blank" href="https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/">https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/</a></li>
            <li><a target="_blank" href="https://arxiv.org/pdf/1604.07316v1.pdf">https://arxiv.org/pdf/1604.07316v1.pdf</a></li>
            <li><a target="_blank" href="https://www.quora.com/Artificial-Intelligence-What-is-an-intuitive-explanation-of-how-deep-Q-networks-DQN-work">https://www.quora.com/Artificial-Intelligence-What-is-an-intuitive-explanation-of-how-deep-Q-networks-DQN-work</a></li>
            <li><a target="_blank" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf</a></li>
            <li><a target="_blank" href="https://arxiv.org/abs/1509.02971">https://arxiv.org/abs/1509.02971</a></li>
        </ul>
    </li>
</ul>
<h3>Suggested Exercises</h3>
<p>Complete some of the exercises in this <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningTutorials/blob/master/ReinforcementLearning.ipynb">Jupyter Notebook</a></p>

<h2>Reinforcement Learning Validation</h2>

<p>Reinforcement learning problems are varied enough that there aren't really standard validation metrics the way there are for things like regression or classification. Typically, the validation is done using the reward -- perhaps the average reward for the last N trials or the change in the reward over time.</p>

<p>If there is a known maximum possible reward, you could use the <strong>regret</strong>, the difference between the maximum reward and the actual reward that the model obtained.</p>
