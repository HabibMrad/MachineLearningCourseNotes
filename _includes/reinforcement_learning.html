<h1>Reinforcement Learning</h1>
<p><strong>Reinforcement learning</strong> is somewhere in between supervised and unsupervised learning. It is meant to work with problems where we have an overall goal we want to get to, but we need the computer to learn lots of intermediate steps to get there, and we are not sure what those intermediate steps are.</p>

<p>What we want is to give our model a representation of the current state of a system and have it choose the best action given that state. We compute the next state of the system given that action and give the model a reward based on that new state. That is, we don't tell the model what answer we want (as we do in supervised learning), but we do tell it how much we liked its answer. It uses this reward (positive or negative) to learn what actions to take.</p>

<p>Assigning the reward for an action can be tricky. For instance, if the model is learning to play checkers, we give it the current state of the board and have it pick among the legal moves available. If this move makes it win the game, we know that it gets a positive reward. If it makes it lose the game, we know that it gets a negative reward. Most of the time, however, we don't know yet what reward to give. We want the model to learn what actions taken in a particular state should get credit for the final reward, since it was certainly not the last action alone that did it. This is known as the <strong>credit assignment problem</strong>.</p>

<p>One way to think about reinforcement learning is that we are trying to learn a function <em>Q</em> that takes in two inputs -- the current state of the world and a particular action -- and returns the <em>quality</em> of taking that action from that state. We call this <strong>Q learning</strong>. By reframing the problem as trying to learn the quality of individual actions, we can make this look more like a supervised learning problem, with the targets being the correct quality for the state/action pair.</p>

<p>To gain training data, we start interacting with the environment, taking random actions and receiving rewards until we hit an end state, such as winning or losing a game. This is called a <strong>roll-out</strong> or <strong>episode</strong>. We might play many episodes initially before we start training. This allows us to build up many (state, action -&gt; reward) observations.</p>

<p>Of course, not every action we take gets a reward -- that's part of what makes this a reinforcement learning problem. If this action did not finish the episode, then we set the target quality to be the current reward plus the quality of the new state that the action got us to (at a discount, because this state should not get full credit for the next state's worth).</p>
<p>In other words, for a particular state and action, the target quality is
    <ul>
        <li>if it ended the episode: the reward we obtained from this action</li>
        <li>if the episode was still going: the reward we obtained from this action + discount * max(current quality of each action possible from the next state)</li>
    </ul>
</p>

<p>Now, we can randomly pick some of the (state, action -&gt; reward) observations as a batch we'll use to train the model, a technique called <strong>experience replay</strong>. Randomly choosing the order can help the model train better, so that it is seeing a set that represents more of the landscape of the entire problem and not a collection of similar states, the way it would if we grouped our training data chronologically.</p>

<p>We continue playing through episodes, choosing actions based on our improved understanding of the environment and using the results to continue to train our model. After training on enough observations, the model learns to value the actions well, and that the action with the highest quality for a particular state is the best one to take.</p>

<p>When multi-layer neural networks are used to learn the Q function, we call this technique <strong>Deep Q Learning</strong>.</p>

<p>Note: with neural networks, it is often easier not to have the action as an input and instead to output a quality score for each action. In this case, when training, the target quality for any action that wasn't taken in the observation is just set to be whatever the model currently thinks it should be.</p>

<h3>Further Resources:</h3>
<ul>
    <li><a target="_blank" href="http://kvfrans.com/reinforcement-learning-basics/">Reinforcement Learning Basics by Kevin Frans</a></li>
    <li><a target="_blank" href="https://www.youtube.com/watch?v=e3Jy2vShroE">[6 min] Intro to Reinforcement Learning</a></li>
    <li><a target="_blank" href="https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT">[1.5 hours] Intro to Reinforcement Learning</a> (There is a lot more math and technical details in this video.)</li>
    <li><a target="_blank" href="https://www.youtube.com/watch?v=79pmNdyxEGo&amp;t=9s">[9 min] Deep Q-Learning for Playing Video Games</a></li>
    <li><a target="_blank" href="https://www.youtube.com/watch?v=Ih8EfvOzBOY">[3 min] Google DeepMind's Deep Q-Learning</a></li>
    <li>Solving Cartpole (a classic reinforcement learning problem):
        <ul>
            <li><a target="_blank" href="http://kvfrans.com/simple-algoritms-for-solving-cartpole/">Simple Algorithms for Solving Cartpole </a></li>
            <li><a target="_blank" href="https://keon.io/deep-q-learning/">Deep Q-Learning with Keras and Gym</a></li>
            <li><a target="_blank" href="https://medium.com/@gtnjuvin/my-journey-into-deep-q-learning-with-keras-and-gym-3e779cc12762">A Journey Into Deep Q-Learning with Keras and Gym</a></li>
            <li><a target="_blank" href="https://github.com/carlshan/intro_to_machine_learning/blob/master/lessons/Reinforcement_Learning/RL_Tutorial.md">Carl's tutorial on solving Cartpole</a></li>
        </ul>
    </li>
    <li><a target="_blank" href="http://karpathy.github.io/2016/05/31/rl/">Deep Reinforcement Learning to Play Pong</a></li>
    <li><a target="_blank" href="https://www.nervanasys.com/demystifying-deep-reinforcement-learning/">Deep Reinforcement Learning to Play Breakout aka Brick Breaker</a></li>
    <li><a target="_blank" href="https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html">Keras Deep Q Learning to Play Flappy Bird</a></li>
    <li>Here are some additional resources recommended by a speaker who visited a previous class:
        <ul>
            <li><a target="_blank" href="https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/">https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/</a></li>
            <li><a target="_blank" href="https://arxiv.org/pdf/1604.07316v1.pdf">https://arxiv.org/pdf/1604.07316v1.pdf</a></li>
            <li><a target="_blank" href="https://www.quora.com/Artificial-Intelligence-What-is-an-intuitive-explanation-of-how-deep-Q-networks-DQN-work">https://www.quora.com/Artificial-Intelligence-What-is-an-intuitive-explanation-of-how-deep-Q-networks-DQN-work</a></li>
            <li><a target="_blank" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf</a></li>
            <li><a target="_blank" href="https://arxiv.org/abs/1509.02971">https://arxiv.org/abs/1509.02971</a></li>
        </ul>
    </li>
</ul>
<h3>Suggested Exercises</h3>
<p>Complete some of the exercises in this <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningTutorials/blob/master/ReinforcementLearning.ipynb">Jupyter Notebook</a></p>

<h2>Reinforcement Learning Validation</h2>

<p>Reinforcement learning problems are varied enough that there aren't really standard validation metrics the way there are for things like regression or classification. Typically, the validation is done using the reward -- perhaps the average reward for the last N trials or the change in the reward over time.</p>

<p>If there is a known maximum possible reward, you could use the <strong>regret</strong>, the difference between the maximum reward and the actual reward that the model obtained.</p>
