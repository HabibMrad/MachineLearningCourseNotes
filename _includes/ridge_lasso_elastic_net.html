<h2>Ridge, LASSO, and Elastic-Net Regression</h2>
<p><strong>Note: you don't have to understand all (or any) of this section. This is extra material for those who want to learn more about forms of linear regression.</strong></p>
<p>When we use ordinary least squares regression (OLS), sometimes we have too many inputs or our estimates of the parameters in our linear model are too large, because we are overfitting to some of the specific examples in our training data. A lot of the inputs may be closely correlated as well, which leads to a number of problems.</p>
<p>As a result, people have invented ways to algorithmically pick "good inputs". This is a <strong>feature selection</strong> problem, and these types of techniques are known as <strong>regularization</strong>.</p>
<p>LASSO and Ridge are both techniques used in linear modeling to select the subset of inputs that are most helpful in predicting the output. (LASSO stands for Least Absolute Shrinkage and Selection Operator.)</p>
<p>Ridge and LASSO have the following in common: they enhance the cost function in OLS with a small addendum -- they add a penalization term to the cost function that pushes the model away from large parameter estimates and from having many parameters with non-zero values.</p>
<p>The specific way that this penalization occurs is slightly different between Ridge and LASSO. In Ridge, you change the cost function so that it also takes into account the sum of the squares of each weight, also known as the <strong>L2 norm</strong> of the parameter vector. The boundary drawn is a circle, and you want to be on the "ridge" of the circle when minimizing your cost function. In LASSO, you penalize with the sum of absolute values of the weights, known as the <strong>L1 norm</strong>. The boundary is a square.</p>
<p>You can learn a lot more about Ridge Regression through <a href="https://www.youtube.com/embed/5asL5Eq2x0A">this 16-minute video</a>.</p>
<p>The penalization term will look like a Lagrange multiplier (if you've seen that before). It'll be &lt;some penalization value you choose&gt;* (the L2 norm of the estimate of your parameters).</p>
<p>While Ridge regression will decrease the variance of your the estimates of your model parameters, it will no longer be unbiased, because we are deliberately asking it to choose parameter values that do not fit the exact dataset as well, in the hopes of having it transfer more easily to new data in the future. The bias-variance tradeoff is something that we will discuss in future classes.</p>
<p>LASSO will add inputs in one at a time, and if the new input doesn't improve the predicted output more than the penalty term, it'll set the weight of the input to zero.</p>
<p>Here's the same video creator explaining Lasso in <a href="https://www.youtube.com/embed/jbwSCwoT51M">a 7-minute video</a>.</p>
<p>In the Elastic-Net version of linear regression, you combine the two penalization terms in some fractional amount (where the two fractions add up to 100%, so for example you can have 30% of the LASSO penalization and 70% Ridge).</p>
