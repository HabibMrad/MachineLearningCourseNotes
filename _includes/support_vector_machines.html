<h2>Support Vector Machines</h2>
<p>Support vector machine, or SVMs, are a common classification method used in binary classification.</p>
<p>The goal of SVMs is to construct a linear decision boundary (which is often called a hyperplane) that separates two classes. However, the technique doesn't just pick any hyperplane that separates two classes of points. After all, there may be
  many potential hyperplanes that could cleanly separate your two classes. Examples of decision boundaries are below:</p>
<p><img src="http://i.imgur.com/5a8hJvw.png" alt="Uncentered decision boundaries" width="670"></p>
<p>Source: <a target="_blank" href="http://www.med.nyu.edu/chibi/sites/default/files/chibi/Final.pdf">http://www.med.nyu.edu/chibi/sites/default/files/chibi/Final.pdf</a> </p>
<p>So the question is: of all possible hyperplanes, which is the best? The intuition behind SVMs is that they construct a hyperplane that has the largest distance (or "margin" or "gap") between the closest members of both classes. An image
  representing this idea is below:</p>
<p><img src="http://i.imgur.com/Zvekw9Dr.png" alt="" width="650"></p>
<p>Source:<strong> </strong><a target="_blank" href="http://www.med.nyu.edu/chibi/sites/default/files/chibi/Final.pdf">http://www.med.nyu.edu/chibi/sites/default/files/chibi/Final.pdf</a> </p>
<p>The vectors describing the datapoints on the boundaries are called "support vectors", which give SVMs their name.</p>
<h3>How do you optimize for finding the "best" hyperplane?</h3>
<p>The equation describing a plane is \(w * x + b = 0\). The 'b' parameter moves the plane and w controls the "orientation" of the plane.</p>
<p>The distance between parallel hyperplanes is \(\frac{abs(b1 - b2)}{length(w)}\) where \(abs\) is the absolute value function. (For those of you who have seen some linear algebra before: \(length(w)\) is the L2 norm of the vector w.)</p>
<p>If you want to maximize the "gap" between different classes, you want to minimize \(length(w)\). You can do so by using techniques from a branch of mathematics called quadratic programming. </p>
<h3>The Kernel Method</h3>
<p>So here's an important question: what if your data can't be separable by a flat hyperplane? One solution you can try is to <em>transform</em> your data so that your data becomes separable once transformed. You do so by applying a <strong>kernel
    function</strong> (<a target="_blank" href="https://en.wikipedia.org/wiki/Kernel_(statistics)">read more on Wikipedia</a>) to your input data, transforming them so that the data is now linearly separable.</p>
<p><img src="http://i.imgur.com/IsB9ojN.png" alt="" width="670"></p>
<p>Source: <a target="_blank" href="http://www.med.nyu.edu/chibi/sites/default/files/chibi/Final.pdf">http://www.med.nyu.edu/chibi/sites/default/files/chibi/Final.pdf</a> </p>
<p>In this case, the final classification is given by whether the distance from the decision boundary is positive or negative.</p>
<h3>Things to ponder:</h3>
<ul>
  <li>SVMs are used for binary classification problems in which you have two classes you're trying to separate. What are some ideas that you have about how you can use SVMs to separate multiple-classes?</li>
  <li>What is the output of SVMs, and how does that differ from Logistic Regression? What are the advantage and disadvantage of using the SVM output to classify points vs. Logistic Regression?</li>
</ul>
<h3>Additional Resources:</h3>
<ul>
  <li><a target="_blank" href="http://www.med.nyu.edu/chibi/sites/default/files/chibi/Final.pdf%20">An introduction to SVMs (with quite a bit of math)</a></li>
  <li><a target="_blank" href="http://docs.opencv.org/2.4/doc/tutorials/ml/introduction_to_svm/introduction_to_svm.html">Introduction with illustrations and code</a></li>
  <li><a target="_blank" href="https://onionesquereality.wordpress.com/2009/03/22/why-are-support-vectors-machines-called-so/">Explanation of the name "SVM"</a></li>
  <li><a target="_blank" href="http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf">Lecture slides -- but ignore the title of the talk; this is not a simple concept</a></li>
  <li><a target="_blank" href="http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html">Some example code in scikit-learn that analyses the iris dataset</a></li>
</ul>
