<h2>Validation</h2>
<p>In our look into <a target="_blank" href="https://nbviewer.jupyter.org/github/jennselby/MachineLearningCourseNotes/blob/master/assets/ipynb/LinearRegression.ipynb">linear regression</a>, when trying to understand our models and whether or not they worked, we used two methods:
<ul>
    <li>We compared our model's learned parameters to the parameters we had used when generating the data.</li>
    <li>We looked at the predictions for a few data points.</li>
</ul>
The first technique is not an option for actual data analyses, where we are using real data instead of synthetically generated data. The second method can be good for our own intuition, but it is <em>not</em> a good way to assess if our model is adequate for the overall task at hand. We need better ways to measure model performance.</p>
<h3>Underfitting and Overfitting</h3>
<p>There are two main things we are concerned about when trying to decide if we have a well-trained model.</p>
<p><strong>Underfitting</strong> means we didn't learn much about our data and cannot predict or describe it well. Maybe our model just doesn't match (e.g. we used a linear model and the relationship between the input and output is quadratic). Maybe we didn't have much data to learn from. Maybe our optimization algorithm failed in some way. Maybe we stopped the training process too early (an issue that can arise with some of the algorithms we'll look at later).</p>
<p><strong>Overfitting </strong>happens when we can predict our training data really, really well, but if we try to predict any new observations, we don't do so well. Basically, we learned how to copy our data rather than learning some general rules. We would say that our model doesn't <strong>generalize</strong> well. This is like passing a multiple-choice test by memorizing the answers "a, a, c, d, d, a, b" instead of knowing why those were the right answers.</p>
<p><img src="https://4.bp.blogspot.com/-6Rcdu0lWFGw/U2TuLyAJC4I/AAAAAAAAEoI/hUmT2S8y2Lo/s1600/model_fit~terms.png" alt="Graph of model fit with different degree polynomials" width="800"/></p>
<p>This image shows different possible models that have been trained on the points in the graph. The straight line is an underfit -- this does not look like a simple linear relationship. However, some of the higher-degree lines look like overfit -- the points at the bottom left probably don't actually suggest that the model needs to dip down there. A really severe overfit would draw a curve that passed through every single point.</p>
<h3>The Bias-Variance Tradeoff</h3>
<p><img src="assets/img/bias_variance.png" alt="bias/variance bullseyes"/></p>
<p><em><a target="_blank" href="http://scott.fortmann-roe.com/docs/BiasVariance.html">Image from Scott Formann-Roe's website</a></em></p>
<p>The underfitting/overfitting dilemma relates to something called the Bias-Variance tradeoff. If a model is heavily biased, that means it makes predictions that tend to be far away from the underlying "true" distribution of the data. In this sense, we can say that it has "underfit" the data.</p>
<p>However, if we build a model that "overfits" to the data at hand, it may generalize poorly to new data we example. We might say that our model has high variance because the exact model parameters will differ greatly depending on the training data at hand.</p>
<p><a target="_blank" href="http://scott.fortmann-roe.com/docs/BiasVariance.html">Read more about the Bias-Variance tradeoff</a>.</p>
<h3>Creating a Test Set</h3>
<p>In order to figure out whether or not our model is overfitting, we need to use some new data that it didn't get to see when it trained. If it predicts this new data well, then we can be more confident that what it learned it training does in fact apply more broadly. We can this new data the <strong>test set</strong>.</p>
<p>In some cases, new data continues to come in, giving us a natural set with which to test how our model handles new data that it didn't train on. In many other cases, however, we have just one (possibly small) data set. What can we do to test for overfitting?</p>
<p>One answer is <strong>holdout</strong>, in which you randomly divide your data into training data and testing data. How big should each part be? Unfortunately, <a target="_blank" href="http://fastml.com/how-much-data-is-enough/">this is not an easy question to answer</a>. It depends on the data you have, the model you're using, your cost function, how you're optimizing, and so on. In general, the more complex your model is, the more data you need -- in both training and testing. However, <a class="jump-to-section" href="#regularization-ridge-lasso-and-elastic-net">regularization</a> can help a model perform better even with a small training set.</p>
<p>If you do not have enough data to make good sized training and test sets, you can try using <strong>cross validation</strong>. In this case, you split your data into several equal parts, enough parts that if you left just one of them out, you'd have enough data left over to train your model. Now, for each part, train the model on all of the data not in that part and test the model on that part. This can give you a big enough "test set" overall that you can make a decisions about which model to use (and then perhaps train it on your entire set).</p>
<p><a target="_blank" href="http://research.cs.tamu.edu/prism/lectures/iss/iss_l13.pdf">See this lecture for more information about creating test sets for validation</a>.</p>

<h2>Validation Metrics</h2>

<p>Once you have created a test set, you'll want some way to score how your model did. What measurement to use depends on what type of problem you're working on.</p>

<ul>
    <li><a class="jump-to-section" href="#regression-validation">Regression</a></li>
    <li><a class="jump-to-section" href="#classification-validation">Classification</a></li>
    <li><a class="jump-to-section" href="#clustering-validation">Clustering</a></li>
    <li><a class="jump-to-section" href="#generative-validation">Generation</a></li>
</ul>
